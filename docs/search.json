[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "JKD Blog",
    "section": "",
    "text": "Hi, I am Dr.¬†Jitendra Kumar Dhiman. I am a PhD in Speech and audio processing. I am passionate about audio technology and AI/ML."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Gumbel-Max Trick\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\n\n\n\n\n\nK-Means Algorithm\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/K_means/K-means.html",
    "href": "posts/K_means/K-means.html",
    "title": "K-Means Algorithm",
    "section": "",
    "text": "The K-means algorithm is an unsupervised learning algorithm that partitions a dataset into k clusters. It works by iteratively assigning data points to clusters and then updating the cluster centroids. A cluster centroid is a representative vector of the cluster in the statistical sense.\nTask: To begin with, we are given a set of data points \\(\\{x_n\\}_{n=1}^N\\), \\(x_n \\in \\mathbb{R}^d\\). Our task is to group them into the given number of clusters \\(K\\).\nSteps: 1. Initialize the cluster centroids:\n\\(\\quad\\) \\(\\bullet\\) Randomly select \\(K\\) data points as the initial cluster centroids.\n2. Assign data points to clusters:\n\\(\\quad\\) \\(\\bullet\\) For each data point, calculate the distance to each cluster centroid.\n\\(\\quad\\) \\(\\bullet\\) Assign the data point to the cluster with the closest centroid.\n3. Update the cluster centroids:\n\\(\\quad\\) \\(\\bullet\\) For each cluster, calculate the mean of all data points assigned to it in the previous step.\n\\(\\quad\\) \\(\\bullet\\) Set the cluster centroid to the calculated mean.\n4. Repeat steps 2 and 3 until convergence:\n\\(\\quad\\) \\(\\bullet\\) Convergence is reached when the cluster centroids no longer change or when a maximum number of iterations is reached."
  },
  {
    "objectID": "posts/K_means/K-means.html#maths-behind-it",
    "href": "posts/K_means/K-means.html#maths-behind-it",
    "title": "K-Means Algorithm",
    "section": "Maths Behind it",
    "text": "Maths Behind it\nLet‚Äôs translate these steps into the maths behind it.\nNotations:\n\\(\\quad\\) \\(\\bullet\\) Let \\(C_k\\) denotes the label of the \\(k^{th}\\) cluster, \\(k \\in \\{1, 2, \\dots, K\\}\\)\n\\(\\quad\\) \\(\\bullet\\) Consider a binary variable \\[r_{nk} = \\begin{cases} 1, & \\text{if } x_n \\text{ is assigned to cluster } C_k, \\\\ 0, & \\text{ otherwise.} \\end{cases}\\]\nThe optimization problem for K-means clustering is defined as follows.\n\\[\\begin{align}\n\\underset{r_{nk}, \\mu_k}{\\text{arg min}} \\sum_{k=1}^K \\sum_{n=1}^{N} r_{nk}\\| x_n - \\mu_k \\|_{2}^2 \\tag{1}\n\\end{align}\\]\nwhere \\(\\mu_k\\) denotes the statistical mean of the data points belonging to the \\(k^{th}\\) cluster also referred to as the cluster centroid. This problem is solved in two phases for the unknowns \\(r_{nk}\\) and \\(\\mu_k\\).\nPhase 1:\nFix \\(\\mu_k\\)‚Äôs for all \\(k\\) and determine \\(r_{nk}\\) for each \\(n\\) as follows. \\[\\begin{align}\nr_{nk^*} = \\begin{cases} 1, & k^* = \\underset{j}{\\text{arg min }} \\|x_n - \\mu_j\\|^2, j = 1, 2, \\dots, K \\\\ 0, & \\text{ otherwise.} \\end{cases}\n\\end{align}\\] This phase translates to Step 2 above in the theory part that is the values of \\(r_{nk}\\) representing the cluster assignment are obtained by computing the minimum distance of each of the data points to the cluster centroids (the centroids \\(\\mu_k\\)‚Äôs are initialized to randomly chosen data points).\nPhase 2:\nFix \\(r_{nk}\\)‚Äôs obtained in Phase 1 and optimize for \\(\\mu_k\\). This requires taking the derivative of Eq. (1) with respect to \\(\\mu_j\\) and equating it to zero which gives\n\\[\\begin{align}\n\\mu_j^* = \\frac{\\sum_n r_{nj}x_n}{\\sum_n r_{nj}},~ j=1,2,\\dots, K.\n\\end{align}\\]\nThis translates to Step 3 above. We just computed the mean of the data points which were closest to a particular centroid. This mean value \\(\\mu_j^*\\) represents the updated cluster centroid. Step 4 from the previous section translates to repeating Phase 1 and Phase 2 until convergence. The full algorithm is described as follows.\n\nAlgorithm:\n\nInitialize \\(\\mu_k\\)‚Äôs for each value \\(k\\) by randomly choosing \\(K\\) data points.\nCalculate \\(r_{nk}\\)‚Äôs holding \\(\\mu_k\\)‚Äôs fixed\nCalculate \\(\\mu_k\\)‚Äôs holding \\(r_{nk}\\)‚Äôs fixed\nRepeat 2 and 3 until convergence that is \\(\\|\\mu_k^{(t+1)} - \\mu_k^{(t)}\\| \\approx 0~\\forall k\\) where \\(t\\) denotes the iteration number.\nOutput: \\(\\{\\mu_k^*\\}_{k=1}^K\\) and \\(\\{r_{nk}^*\\}_{k=1}^K\\).\n\nIt should be noted that the algorithm requires prior knowledge of \\(K\\). In practice, this is unknown for a given dataset. There are several methods proposed in the literature to get an estimate of the number of clusters from a given dataset (Please refer or ask ChatGPT üòä!).\nThat‚Äôs all for this algorithm üòä! The next section provides a Python code for it."
  },
  {
    "objectID": "posts/K_means/K-means.html#python-code-for-k-means-algorithm",
    "href": "posts/K_means/K-means.html#python-code-for-k-means-algorithm",
    "title": "K-Means Algorithm",
    "section": "Python Code for K-Means Algorithm",
    "text": "Python Code for K-Means Algorithm\n\n\nCode\n# @title\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nX, y = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=0) # X: N x d where N = 300, d = 2\nmin_x, max_x = min(X[:,0]), max(X[:,0])\nmin_y, max_y = min(X[:,1]), max(X[:,1])\n# Note: you can play with cluster_std to create either well separated clusters or overlapping ones\n\n\n\n\nCode\n# @title\ndef k_means_clustering(X, K, mu):\n  \"\"\" X: N x d\n      K: number of clusters\n      mu: K x d\n  \"\"\"\n  flag = False\n  D = []\n  updated_mu = mu.copy() # make a copy of current centroid\n  # Phase 1: fix mu and find rnk\n  for k in range(K):\n    # compute distance of each of the data points from cluster centriods\n    D += [np.sum((X - mu[k,:])**2, axis = -1)] # each member list has a shape of (K,)\n    # find index of the closest centroid to each of the data point\n    closest_cluster = np.argmin(D, axis = 0) # cluster_class can take any value from [0, K-1]\n    rnk = np.eye(K)[closest_cluster] # construct one-hot -&gt; rnk: n x k\n\n  # Phase 2: for fix rnk update mu\n  for k in range(K):\n    data_k = X[rnk[:, k].astype(bool), :]\n    if data_k.size != 0:\n      updated_mu[k, :] = np.sum(data_k, axis=0) / np.sum(rnk[:, k], axis=0)\n    else:\n      updated_mu[k,:] = mu[k, :]\n  # criterion\n  if np.sum((mu - updated_mu)**2) &lt;= np.finfo(float).eps:\n    print(\"Convergence criterion met, exiting!\")\n    flag = True\n    return rnk, updated_mu, flag\n  return rnk, updated_mu, flag\n\nK = 3\nmax_itr = 10\n# initialize cluster centroids randomly from the given data points\nidx_K = np.random.randint(0, X.shape[0], K)\nmu = X[idx_K, :]\ninitial_mu = mu.copy()\nfor i in range(max_itr):\n  rnk, mu, flag = k_means_clustering(X, K, mu)\n  if flag:\n    break\nif i == max_itr - 1:\n  print('Mximum iterations reached!')\n\n\nConvergence criterion met, exiting!\n\n\n\n\nCode\n# @title\nfig, axs = plt.subplots(1, 2, figsize=(8, 3))\naxs[0].scatter(X[:, 0], X[:, 1], s=2, color='black', label=\"Data\")\naxs[0].scatter(initial_mu[:, 0], initial_mu[:, 1], s=80, color='g', marker='x', label=\"Initial centroids\", linewidths=2)\naxs[0].legend(fontsize='small')\naxs[0].set_title(\"Original Data\")\naxs[0].set_xlim(min_x, max_x)\nc = ['black', 'blue', 'y']\nk = np.arange(K)\nfor i, k in zip(c, k):\n  axs[1].scatter(X[rnk[:, k].astype(bool), 0], X[rnk[:,k].astype(bool), 1], color=i, s=2)\naxs[1].scatter(initial_mu[:, 0], initial_mu[:, 1], s=80, color='g', marker='x', label=\"Initial centroids\", linewidths=2)\naxs[1].scatter(mu[:, 0], mu[:, 1], s=80, color='red', marker='x', label='Final centroids', linewidths=2)\naxs[1].legend(fontsize='small')\naxs[1].set_title(\"Clusters After K-means Convergence\")\naxs[1].set_xlim(min_x, max_x)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/gumbel_max/gumbel_max.html",
    "href": "posts/gumbel_max/gumbel_max.html",
    "title": "Gumbel-Max Trick",
    "section": "",
    "text": "Pre-requisite: Probability theory\n\nThe Gumbel-max trick is a useful technique for sampling from a discrete distribution, especially when built-in statistical functions are unavailable. Although many statistical software packages can handle a wide range of distributions, there are situations where you need to sample from distributions that may not be explicitly supported.\n\nFurther, the Gumbel-Softmax trick (a varient of Gumbel-max trick) enables differentiable sampling from a categorical distribution by adding Gumbel noise to the logits and applying the softmax function, making it possible to train models with discrete variables using gradient-based methods.\n\nIn this blog, we will discuss the Gumbel-max trick, its mathematical proof, and a simulation example. Before diving into the trick, let‚Äôs do something easy first. We will briefly review the inverse cumulative distribution function (CDF) method for sampling from a distribution, which applies to both continuous and discrete distributions. In contrast, the Gumbel-max trick is specifically applicable to the discrete distributions.\n\nInverse CDF method\nDefinition: The CDF of a random variable \\(Z\\) is given by \\(F_Z(z) = P(Z \\leq z)~\\forall~z\\) where \\(P(\\cdot)\\) denotes the probability. The CDF is a non-decreasing function defined on the real line \\(\\mathbb{R}\\). Definition: For a non-decreasing function \\(F\\) on \\(\\mathbb{R}\\), the generalized inverse is defined as \\(F^{-1}(u) = min\\{z: F(z) \\geq u\\}, u \\in \\mathbb{R}\\). For a continuous random variable the inequality ‚Äú\\(\\geq\\)‚Äù become equality ‚Äú\\(=\\)‚Äù. Suppose that we want to sample \\(Z \\sim P(z)\\), and we have access to inverse CDF of \\(Z\\), then Lemma: If \\(U \\sim \\text{Uniform}(0, 1)\\), then \\(Z = F_Z^{-1}(U)\\) is a simulation from \\(P(z)\\). Proof: Consider \\(P(Z \\leq z) = P(F_Z^{-1}(U) \\leq z) = P(U \\leq F_Z(z)) = \\int_{0}^{F_Z(z)} 1~\\text{d}u =  F_Z(z)\\). Hence, \\(Z\\) follows the desired distribution \\(P(z)\\). We can observe that the inverse CDF method can be used to generate samples of \\(Z\\) through the transformation of a uniform random variable where the transformation is the inverse CDF of \\(Z\\) itself. For most of the statistical software, it is easier to generate samples from a uniform distribution.\n\nA Coding example\nSuppose \\(Z\\) is a categorical random variable that is \\(Z\\) can take values from \\(\\{1, 2, \\dots, K\\}\\) with probabilities \\(P(Z=k)=p_k\\) where \\(K\\) denotes the categories. We would like to generate various realizations (samples) of \\(Z\\) (e.g.¬†\\(\\{Z_1, Z_2, \\cdots, Z_n\\}\\)) from this distribution. We can employ the inverse CDF method. Please refer to the Python code example for more clarity, come back, and read again.\n\n\n\nGumbel-max trick\nDefinition: The Gumbel distribution of a random variable \\(X \\in \\mathbb{R}\\) is given by \\(f_X(x) = \\frac{1}{\\beta}\\exp(-(x - \\mu) - \\exp(-(x - \\mu)/\\beta))\\), the CDF is \\(F_X(x) = P(X \\leq x) = \\exp(-\\exp(-(x - \\mu)/\\beta))\\), and the inverse CDF is \\(-\\log(\\log(x - \\mu)/\\beta)\\). To the scope of this discussion, it is enough to consider \\(\\mu=0, \\beta=1\\), that is \\(X \\sim \\text{Gumbel}(0, 1)\\). The Gumbel-max trick converts the sampling problem into an optimization problem and the key result is as follows. \\[\\begin{align}\n\\tag{1}\nI = \\underset{k}{\\text{argmax}}~(\\log(p_k) + G_k) \\sim \\text{Categorical}(p_k), 1 \\leq k \\leq K\n\\end{align}\\] where \\(G_k \\sim \\text{Gumbel}(0, 1)\\) are Gumbel-distributed i.i.d random variables, and \\(I\\in \\mathbb{Z}\\) is a random variable because of \\(G_k\\). Eq. (1) states that the index \\(I\\) follows the desired categorical distribution (see proof), this index is obtained by adding samples from the Gumbel distribution (the Gumbel noise) to the log probabilities and choosing the index for which the quantity is maximum.  Proof: Let \\(X_k = \\log(p_k) + G_k \\in \\mathbb{R}\\). If the solution of Eq. (1) is the index \\(k\\), then the event \\(\\{I = k\\}\\) is equivalent to the event \\(\\{X_k &gt; X_j~\\forall j \\neq k\\}\\) in the probability space. We can further translate it to another equivalent event ‚Äú\\(\\{\\text{sum over all}~x~(X_k = x, X_j &lt; x~\\forall j \\neq k)\\}\\)‚Äù. Please note that \\(G_k\\)‚Äôs are i.i.d and consequently \\(X_k\\)‚Äôs are also i.i.d. Hence in probability space, we have \\[\\begin{align}\nP(I = k) &= P(X_k &gt; X_j~\\forall k \\neq j) = \\int P(X_k = x, X_j &lt; x~\\forall j \\neq k)~\\mathrm{d}x \\\\ \\nonumber\n&=\\int P(X_k = x) P(X_j &lt; x~\\forall~j \\neq k|X_k = x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(X_k = x) P(X_j &lt; x~\\forall~j \\neq k)~\\mathrm{d}x =  \\int P(X_k = x) \\prod_{j \\neq k} P(X_j &lt; x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(G_k = x - log(p_k)) \\prod_{j \\neq k} P(G_j &lt; x - \\log(p_j))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k) - x - \\exp(\\log(p_k) - x)) \\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x ~(\\text{using pdf and cdf of } G_k) \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k))\\exp(-x)\\exp(-\\exp(\\log(p_k) - x))\\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x)\\prod_{k} \\exp(-\\exp(\\log(p_k) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x) \\prod_{k} \\exp(-p_k\\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int \\exp(-x) \\exp(-\\sum_{k} p_k \\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int_{-\\infty}^{+\\infty} \\exp(-x) \\exp(-1\\exp(-x)))~\\mathrm{d}x = p_k\n\\end{align}\\] This completes the proof. Hence the random variable \\(I \\sim \\text{Cat}(p_k)\\). To generate samples using this method, you only need to sample Gumbel-distributed variables \\(G_k\\) and find the index that maximizes \\(\\log(p_k) + G_k\\). In contrast to inverse CDF method, this avoids computing the CDF or performing a binary search. To sample Gumbel-distributed variables, you can start with a uniform random variable and transform it using the inverse Gumbel CDF \\((-\\log(-\\log(-x)))\\) which is a closed-form expression, the resulting random variable will follow the Gumbel distribution. We can now generate various realizations of \\(I\\), i.e.¬†\\(\\{I_1, I_2, \\dots, I_n\\}\\) by adding each time a different realization of Gumbel noise to the log-probabilities and picking up on the maximum index. Please refer to the Python code.\n\n\nApplication to deep learning\nThe Gumbel-max trick finds significant application in deep learning, particularly in sampling from categorical distributions during optimization. Below are two common issues in deep learning that the trick helps address.\n\nIssue-1\nOptimizing categorical probabilities \\(p_k\\) often requires solving a constrained problem where \\(\\sum_k p_k = 1\\) which is hard to optimize using gradient descent. To overcome this, one simple trick is to reparameterize \\(p_k\\) using another variable \\(\\theta_k \\in \\mathbb{R}\\) through softmax.\n\n\\[\\begin{align}\np_k = \\frac{\\exp(\\theta_k)}{\\sum_j \\exp(\\theta_j)}~\\mathrm{where}~\\theta_k \\in \\mathbb{R}.\n\\end{align}\\]\n\\(\\theta_k\\) could be the logits from a neural network. This formulation allows for unconstrained optimization since the logits can take any real value. However, in some scenarios, normalization can be either computationally expensive or may not be possible. You can think of a streaming automatic speech recognition system, where not all the logits are available. Gumbel-Max trick can overcome this. Following the proof along the similar lines as above, it can be shown that \\(\\underset{k}{\\text{argmax}}\\{\\theta_k + G_k\\} \\sim \\text{Cat}(p_k)\\) (You can do it as an exercise). This result remains valid for each \\(\\theta_k\\)‚Äôs.\n\nIssue-2\nNeural networks typically rely on gradient-based optimization, which requires all operations to be differentiable. Both the inverse CDF method and the Gumbel-max trick involve non-differentiable operations.\n\nThe inverse CDF method introduces discontinuities when selecting categories based on cumulative probabilities.\nThe Gumbel-max trick involves the \\(\\text{argmax}\\) operation, which is non-differentiable.\n\n\nWhile many researchers have proposed relaxation for the differentiability of both methods, the Softmax variant of the Gumbel-max trick ‚ÄúGumbel-softmax trick‚Äù is often used. It replaces the \\(\\text{argmax}\\) with a differentiable approximation (softmax) that allows gradients to flow through the sampling process, making it suitable for backpropagation during neural network training. We leave this for future discussion (reference: Gumbel-Softmax).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\n# Inverse CDF Method\ndef inverse_cdf(prob):\n    u = np.random.uniform()\n    cdf_of_z = np.cumsum(prob)\n    # using generalized inverse, we need to find the minimum index j such that F(j) &gt;= u for all u, for this we use numpy function \"searchsorted\"\n    sample = np.searchsorted(cdf_of_z, u) # this yields the samples of Z\n    return sample\n# Gumbel-max trick\ndef gumbel_max(prob):\n    log_p = np.log(prob)\n    u = np.random.uniform(0.0, 1.0, len(log_p))\n    G = -np.log(-np.log(u)) # use inverse Gumbel CDF to sample from Gumbel distribution given the uniform random variables\n    I = np.argmax(prob + G)\n    return I\n\ndef est_prob(samples, n_samples):\n    return np.bincount(samples) / n_samples\n\nprob = [0.1, 0.2, 0.3, 0.4] # the pk's for K = 4\nn_samples = 5000\nstart_g = time()\nsamples_gumbel = [gumbel_max(prob) for _ in range(n_samples)] # generate samples using Gumbel\nend_g = time() - start_g\nprint(\"--\"*20)\nprint(f\"Time taken by Gumbel-max: {end_g:0.4f} Sec\")\nstart_cdf = time()\nsamples_invcdf = [inverse_cdf(prob) for _ in range(n_samples)] # generate samples using inverse CDF \nprint(f\"Time taken by inverse CDF: {time() - start_cdf:0.4f} Sec\")\n# we can compute the estimated probabilites from the samples of Z\nest_prob_gumbel = est_prob(samples_gumbel, n_samples)\nest_prob_invcdf =  est_prob(samples_invcdf, n_samples) # more number of samples (n_samples) will give better estimates\nerror_gumbel = abs(prob - est_prob_gumbel)\nerror_invcdf = abs(prob - est_prob_invcdf)\n\nbar_width = 0.1\nK = np.arange(len(prob)) # number of categories\nfig, ax = plt.subplots(2,1)\nax[0].bar(K - bar_width, prob, bar_width, label=\"ORG. PROB.\")\nax[0].bar(K, est_prob_invcdf, bar_width, color='brown', label=\"EST. PROB. (Inv CDF)\")\nax[0].bar(K + bar_width, est_prob_gumbel, bar_width, color='green', label=\"EST. PROB. (Gumbel)\")\nax[0].set_ylim([0, 1])\nax[0].set_ylabel(\"PROBABILITIES\")\nax[1].bar(K, error_invcdf , bar_width, color='brown', label=\"ERROR (Inv CDF)\")\nax[1].bar(K + bar_width, error_gumbel, bar_width, color='green', label=\"ERROR (Gumbel)\")\nax[1].set_xlabel(\"CATEGORIES\")\nax[1].set_ylabel(\"ERROR\")\nax[0].legend()\nax[1].legend()\nprint(f\"Avg. error Gumbel-max: {np.sum(error_gumbel)/len(prob):0.4f}\")\nprint(f\"Avg. error inv CDF: {np.sum(error_invcdf)/len(prob):0.4f}\")\n\n\n----------------------------------------\nTime taken by Gumbel-max: 0.4369 Sec\nTime taken by inverse CDF: 0.3250 Sec\nAvg. error Gumbel-max: 0.0753\nAvg. error inv CDF: 0.0050"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JKD Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nGumbel-Max Trick\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\n6 min\n\n\n\n\n\n\n\nK-Means Algorithm\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  }
]