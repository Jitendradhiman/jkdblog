[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "JKD Blog",
    "section": "",
    "text": "Hi, I am Dr. Jitendra Kumar Dhiman. I am a PhD in Speech and audio processing. I am passionate about audio technology and AI/ML."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Variational Autoencoder\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n\n\n\n\n\nGumbel-Max Trick\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\n\n\n\n\n\nK-Means Algorithm\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gumbel_max/gumbel_max.html",
    "href": "posts/gumbel_max/gumbel_max.html",
    "title": "Gumbel-Max Trick",
    "section": "",
    "text": "Pre-requisite: Probability theory\n\nThe Gumbel-max trick is a useful technique for sampling from a discrete distribution, especially when built-in statistical functions are unavailable. Although many statistical software packages can handle a wide range of distributions, there are situations where you need to sample from distributions that may not be explicitly supported.\n\nFurther, the Gumbel-Softmax trick (a varient of Gumbel-max trick) enables differentiable sampling from a categorical distribution by adding Gumbel noise to the logits and applying the softmax function, making it possible to train models with discrete variables using gradient-based methods.\n\nIn this blog, we will discuss the Gumbel-max trick, its mathematical proof, and a simulation example. Before diving into the trick, let’s do something easy first. We will briefly review the inverse cumulative distribution function (CDF) method for sampling from a distribution, which applies to both continuous and discrete distributions. In contrast, the Gumbel-max trick is specifically applicable to the discrete distributions.\n\nInverse CDF method\nDefinition: The CDF of a random variable \\(Z\\) is given by \\(F_Z(z) = P(Z \\leq z)~\\forall~z\\) where \\(P(\\cdot)\\) denotes the probability. The CDF is a non-decreasing function defined on the real line \\(\\mathbb{R}\\). Definition: For a non-decreasing function \\(F\\) on \\(\\mathbb{R}\\), the generalized inverse is defined as \\(F^{-1}(u) = min\\{z: F(z) \\geq u\\}, u \\in \\mathbb{R}\\). For a continuous random variable the inequality “\\(\\geq\\)” become equality “\\(=\\)”. Suppose that we want to sample \\(Z \\sim P(z)\\), and we have access to inverse CDF of \\(Z\\), then Lemma: If \\(U \\sim \\text{Uniform}(0, 1)\\), then \\(Z = F_Z^{-1}(U)\\) is a simulation from \\(P(z)\\). Proof: Consider \\(P(Z \\leq z) = P(F_Z^{-1}(U) \\leq z) = P(U \\leq F_Z(z)) = \\int_{0}^{F_Z(z)} 1~\\text{d}u =  F_Z(z)\\). Hence, \\(Z\\) follows the desired distribution \\(P(z)\\). We can observe that the inverse CDF method can be used to generate samples of \\(Z\\) through the transformation of a uniform random variable where the transformation is the inverse CDF of \\(Z\\) itself. For most of the statistical software, it is easier to generate samples from a uniform distribution.\n\nA Coding example\nSuppose \\(Z\\) is a categorical random variable that is \\(Z\\) can take values from \\(\\{1, 2, \\dots, K\\}\\) with probabilities \\(P(Z=k)=p_k\\) where \\(K\\) denotes the categories. We would like to generate various realizations (samples) of \\(Z\\) (e.g. \\(\\{Z_1, Z_2, \\cdots, Z_n\\}\\)) from this distribution. We can employ the inverse CDF method. Please refer to the Python code example for more clarity, come back, and read again.\n\n\n\nGumbel-max trick\nDefinition: The Gumbel distribution of a random variable \\(X \\in \\mathbb{R}\\) is given by \\(f_X(x) = \\frac{1}{\\beta}\\exp(-(x - \\mu) - \\exp(-(x - \\mu)/\\beta))\\), the CDF is \\(F_X(x) = P(X \\leq x) = \\exp(-\\exp(-(x - \\mu)/\\beta))\\), and the inverse CDF is \\(-\\log(\\log(x - \\mu)/\\beta)\\). To the scope of this discussion, it is enough to consider \\(\\mu=0, \\beta=1\\), that is \\(X \\sim \\text{Gumbel}(0, 1)\\). The Gumbel-max trick converts the sampling problem into an optimization problem and the key result is as follows. \\[\\begin{align}\n\\tag{1}\nI = \\underset{k}{\\text{argmax}}~(\\log(p_k) + G_k) \\sim \\text{Categorical}(p_k), 1 \\leq k \\leq K\n\\end{align}\\] where \\(G_k \\sim \\text{Gumbel}(0, 1)\\) are Gumbel-distributed i.i.d random variables, and \\(I\\in \\mathbb{Z}\\) is a random variable because of \\(G_k\\). Eq. (1) states that the index \\(I\\) follows the desired categorical distribution (see proof), this index is obtained by adding samples from the Gumbel distribution (the Gumbel noise) to the log probabilities and choosing the index for which the quantity is maximum.  Proof: Let \\(X_k = \\log(p_k) + G_k \\in \\mathbb{R}\\). If the solution of Eq. (1) is the index \\(k\\), then the event \\(\\{I = k\\}\\) is equivalent to the event \\(\\{X_k &gt; X_j~\\forall j \\neq k\\}\\) in the probability space. We can further translate it to another equivalent event “\\(\\{\\text{sum over all}~x~(X_k = x, X_j &lt; x~\\forall j \\neq k)\\}\\)”. Please note that \\(G_k\\)’s are i.i.d and consequently \\(X_k\\)’s are also i.i.d. Hence in probability space, we have \\[\\begin{align}\nP(I = k) &= P(X_k &gt; X_j~\\forall k \\neq j) = \\int P(X_k = x, X_j &lt; x~\\forall j \\neq k)~\\mathrm{d}x \\\\ \\nonumber\n&=\\int P(X_k = x) P(X_j &lt; x~\\forall~j \\neq k|X_k = x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(X_k = x) P(X_j &lt; x~\\forall~j \\neq k)~\\mathrm{d}x =  \\int P(X_k = x) \\prod_{j \\neq k} P(X_j &lt; x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(G_k = x - log(p_k)) \\prod_{j \\neq k} P(G_j &lt; x - \\log(p_j))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k) - x - \\exp(\\log(p_k) - x)) \\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x ~(\\text{using pdf and cdf of } G_k) \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k))\\exp(-x)\\exp(-\\exp(\\log(p_k) - x))\\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x)\\prod_{k} \\exp(-\\exp(\\log(p_k) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x) \\prod_{k} \\exp(-p_k\\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int \\exp(-x) \\exp(-\\sum_{k} p_k \\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int_{-\\infty}^{+\\infty} \\exp(-x) \\exp(-1\\exp(-x)))~\\mathrm{d}x = p_k\n\\end{align}\\] This completes the proof. Hence the random variable \\(I \\sim \\text{Cat}(p_k)\\). To generate samples using this method, you only need to sample Gumbel-distributed variables \\(G_k\\) and find the index that maximizes \\(\\log(p_k) + G_k\\). In contrast to inverse CDF method, this avoids computing the CDF or performing a binary search. To sample Gumbel-distributed variables, you can start with a uniform random variable and transform it using the inverse Gumbel CDF \\((-\\log(-\\log(-x)))\\) which is a closed-form expression, the resulting random variable will follow the Gumbel distribution. We can now generate various realizations of \\(I\\), i.e. \\(\\{I_1, I_2, \\dots, I_n\\}\\) by adding each time a different realization of Gumbel noise to the log-probabilities and picking up on the maximum index. Please refer to the Python code.\n\n\nApplication to deep learning\nThe Gumbel-max trick finds significant application in deep learning, particularly in sampling from categorical distributions during optimization. Below are two common issues in deep learning that the trick helps address.\n\nIssue-1\nOptimizing categorical probabilities \\(p_k\\) often requires solving a constrained problem where \\(\\sum_k p_k = 1\\) which is hard to optimize using gradient descent. To overcome this, one simple trick is to reparameterize \\(p_k\\) using another variable \\(\\theta_k \\in \\mathbb{R}\\) through softmax.\n\n\\[\\begin{align}\np_k = \\frac{\\exp(\\theta_k)}{\\sum_j \\exp(\\theta_j)}~\\mathrm{where}~\\theta_k \\in \\mathbb{R}.\n\\end{align}\\]\n\\(\\theta_k\\) could be the logits from a neural network. This formulation allows for unconstrained optimization since the logits can take any real value. However, in some scenarios, normalization can be either computationally expensive or may not be possible. You can think of a streaming automatic speech recognition system, where not all the logits are available. Gumbel-Max trick can overcome this. Following the proof along the similar lines as above, it can be shown that \\(\\underset{k}{\\text{argmax}}\\{\\theta_k + G_k\\} \\sim \\text{Cat}(p_k)\\) (You can do it as an exercise). This result remains valid for each \\(\\theta_k\\)’s.\n\nIssue-2\nNeural networks typically rely on gradient-based optimization, which requires all operations to be differentiable. Both the inverse CDF method and the Gumbel-max trick involve non-differentiable operations.\n\nThe inverse CDF method introduces discontinuities when selecting categories based on cumulative probabilities.\nThe Gumbel-max trick involves the \\(\\text{argmax}\\) operation, which is non-differentiable.\n\n\nWhile many researchers have proposed relaxation for the differentiability of both methods, the Softmax variant of the Gumbel-max trick “Gumbel-softmax trick” is often used. It replaces the \\(\\text{argmax}\\) with a differentiable approximation (softmax) that allows gradients to flow through the sampling process, making it suitable for backpropagation during neural network training. We leave this for future discussion (reference: Gumbel-Softmax).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\n# Inverse CDF Method\ndef inverse_cdf(prob):\n    u = np.random.uniform()\n    cdf_of_z = np.cumsum(prob)\n    # using generalized inverse, we need to find the minimum index j such that F(j) &gt;= u for all u, for this we use numpy function \"searchsorted\"\n    sample = np.searchsorted(cdf_of_z, u) # this yields the samples of Z\n    return sample\n# Gumbel-max trick\ndef gumbel_max(prob):\n    log_p = np.log(prob)\n    u = np.random.uniform(0.0, 1.0, len(log_p))\n    G = -np.log(-np.log(u)) # use inverse Gumbel CDF to sample from Gumbel distribution given the uniform random variables\n    I = np.argmax(prob + G)\n    return I\n\ndef est_prob(samples, n_samples):\n    return np.bincount(samples) / n_samples\n\nprob = [0.1, 0.2, 0.3, 0.4] # the pk's for K = 4\nn_samples = 5000\nstart_g = time()\nsamples_gumbel = [gumbel_max(prob) for _ in range(n_samples)] # generate samples using Gumbel\nend_g = time() - start_g\nprint(\"--\"*20)\nprint(f\"Time taken by Gumbel-max: {end_g:0.4f} Sec\")\nstart_cdf = time()\nsamples_invcdf = [inverse_cdf(prob) for _ in range(n_samples)] # generate samples using inverse CDF \nprint(f\"Time taken by inverse CDF: {time() - start_cdf:0.4f} Sec\")\n# we can compute the estimated probabilites from the samples of Z\nest_prob_gumbel = est_prob(samples_gumbel, n_samples)\nest_prob_invcdf =  est_prob(samples_invcdf, n_samples) # more number of samples (n_samples) will give better estimates\nerror_gumbel = abs(prob - est_prob_gumbel)\nerror_invcdf = abs(prob - est_prob_invcdf)\n\nbar_width = 0.1\nK = np.arange(len(prob)) # number of categories\nfig, ax = plt.subplots(2,1)\nax[0].bar(K - bar_width, prob, bar_width, label=\"ORG. PROB.\")\nax[0].bar(K, est_prob_invcdf, bar_width, color='brown', label=\"EST. PROB. (Inv CDF)\")\nax[0].bar(K + bar_width, est_prob_gumbel, bar_width, color='green', label=\"EST. PROB. (Gumbel)\")\nax[0].set_ylim([0, 1])\nax[0].set_ylabel(\"PROBABILITIES\")\nax[1].bar(K, error_invcdf , bar_width, color='brown', label=\"ERROR (Inv CDF)\")\nax[1].bar(K + bar_width, error_gumbel, bar_width, color='green', label=\"ERROR (Gumbel)\")\nax[1].set_xlabel(\"CATEGORIES\")\nax[1].set_ylabel(\"ERROR\")\nax[0].legend()\nax[1].legend()\nprint(f\"Avg. error Gumbel-max: {np.sum(error_gumbel)/len(prob):0.4f}\")\nprint(f\"Avg. error inv CDF: {np.sum(error_invcdf)/len(prob):0.4f}\")\n\n\n----------------------------------------\nTime taken by Gumbel-max: 0.4369 Sec\nTime taken by inverse CDF: 0.3250 Sec\nAvg. error Gumbel-max: 0.0753\nAvg. error inv CDF: 0.0050"
  },
  {
    "objectID": "posts/K_means/K-means.html",
    "href": "posts/K_means/K-means.html",
    "title": "K-Means Algorithm",
    "section": "",
    "text": "The K-means algorithm is an unsupervised learning algorithm that partitions a dataset into k clusters. It works by iteratively assigning data points to clusters and then updating the cluster centroids. A cluster centroid is a representative vector of the cluster in the statistical sense.\nTask: To begin with, we are given a set of data points \\(\\{x_n\\}_{n=1}^N\\), \\(x_n \\in \\mathbb{R}^d\\). Our task is to group them into the given number of clusters \\(K\\).\nSteps: 1. Initialize the cluster centroids:\n\\(\\quad\\) \\(\\bullet\\) Randomly select \\(K\\) data points as the initial cluster centroids.\n2. Assign data points to clusters:\n\\(\\quad\\) \\(\\bullet\\) For each data point, calculate the distance to each cluster centroid.\n\\(\\quad\\) \\(\\bullet\\) Assign the data point to the cluster with the closest centroid.\n3. Update the cluster centroids:\n\\(\\quad\\) \\(\\bullet\\) For each cluster, calculate the mean of all data points assigned to it in the previous step.\n\\(\\quad\\) \\(\\bullet\\) Set the cluster centroid to the calculated mean.\n4. Repeat steps 2 and 3 until convergence:\n\\(\\quad\\) \\(\\bullet\\) Convergence is reached when the cluster centroids no longer change or when a maximum number of iterations is reached."
  },
  {
    "objectID": "posts/K_means/K-means.html#maths-behind-it",
    "href": "posts/K_means/K-means.html#maths-behind-it",
    "title": "K-Means Algorithm",
    "section": "Maths Behind it",
    "text": "Maths Behind it\nLet’s translate these steps into the maths behind it.\nNotations:\n\\(\\quad\\) \\(\\bullet\\) Let \\(C_k\\) denotes the label of the \\(k^{th}\\) cluster, \\(k \\in \\{1, 2, \\dots, K\\}\\)\n\\(\\quad\\) \\(\\bullet\\) Consider a binary variable \\[r_{nk} = \\begin{cases} 1, & \\text{if } x_n \\text{ is assigned to cluster } C_k, \\\\ 0, & \\text{ otherwise.} \\end{cases}\\]\nThe optimization problem for K-means clustering is defined as follows.\n\\[\\begin{align}\n\\underset{r_{nk}, \\mu_k}{\\text{arg min}} \\sum_{k=1}^K \\sum_{n=1}^{N} r_{nk}\\| x_n - \\mu_k \\|_{2}^2 \\tag{1}\n\\end{align}\\]\nwhere \\(\\mu_k\\) denotes the statistical mean of the data points belonging to the \\(k^{th}\\) cluster also referred to as the cluster centroid. This problem is solved in two phases for the unknowns \\(r_{nk}\\) and \\(\\mu_k\\).\nPhase 1:\nFix \\(\\mu_k\\)’s for all \\(k\\) and determine \\(r_{nk}\\) for each \\(n\\) as follows. \\[\\begin{align}\nr_{nk^*} = \\begin{cases} 1, & k^* = \\underset{j}{\\text{arg min }} \\|x_n - \\mu_j\\|^2, j = 1, 2, \\dots, K \\\\ 0, & \\text{ otherwise.} \\end{cases}\n\\end{align}\\] This phase translates to Step 2 above in the theory part that is the values of \\(r_{nk}\\) representing the cluster assignment are obtained by computing the minimum distance of each of the data points to the cluster centroids (the centroids \\(\\mu_k\\)’s are initialized to randomly chosen data points).\nPhase 2:\nFix \\(r_{nk}\\)’s obtained in Phase 1 and optimize for \\(\\mu_k\\). This requires taking the derivative of Eq. (1) with respect to \\(\\mu_j\\) and equating it to zero which gives\n\\[\\begin{align}\n\\mu_j^* = \\frac{\\sum_n r_{nj}x_n}{\\sum_n r_{nj}},~ j=1,2,\\dots, K.\n\\end{align}\\]\nThis translates to Step 3 above. We just computed the mean of the data points which were closest to a particular centroid. This mean value \\(\\mu_j^*\\) represents the updated cluster centroid. Step 4 from the previous section translates to repeating Phase 1 and Phase 2 until convergence. The full algorithm is described as follows.\n\nAlgorithm:\n\nInitialize \\(\\mu_k\\)’s for each value \\(k\\) by randomly choosing \\(K\\) data points.\nCalculate \\(r_{nk}\\)’s holding \\(\\mu_k\\)’s fixed\nCalculate \\(\\mu_k\\)’s holding \\(r_{nk}\\)’s fixed\nRepeat 2 and 3 until convergence that is \\(\\|\\mu_k^{(t+1)} - \\mu_k^{(t)}\\| \\approx 0~\\forall k\\) where \\(t\\) denotes the iteration number.\nOutput: \\(\\{\\mu_k^*\\}_{k=1}^K\\) and \\(\\{r_{nk}^*\\}_{k=1}^K\\).\n\nIt should be noted that the algorithm requires prior knowledge of \\(K\\). In practice, this is unknown for a given dataset. There are several methods proposed in the literature to get an estimate of the number of clusters from a given dataset (Please refer or ask ChatGPT 😊!).\nThat’s all for this algorithm 😊! The next section provides a Python code for it."
  },
  {
    "objectID": "posts/K_means/K-means.html#python-code-for-k-means-algorithm",
    "href": "posts/K_means/K-means.html#python-code-for-k-means-algorithm",
    "title": "K-Means Algorithm",
    "section": "Python Code for K-Means Algorithm",
    "text": "Python Code for K-Means Algorithm\n\n\nCode\n# @title\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nX, y = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=0) # X: N x d where N = 300, d = 2\nmin_x, max_x = min(X[:,0]), max(X[:,0])\nmin_y, max_y = min(X[:,1]), max(X[:,1])\n# Note: you can play with cluster_std to create either well separated clusters or overlapping ones\n\n\n\n\nCode\n# @title\ndef k_means_clustering(X, K, mu):\n  \"\"\" X: N x d\n      K: number of clusters\n      mu: K x d\n  \"\"\"\n  flag = False\n  D = []\n  updated_mu = mu.copy() # make a copy of current centroid\n  # Phase 1: fix mu and find rnk\n  for k in range(K):\n    # compute distance of each of the data points from cluster centriods\n    D += [np.sum((X - mu[k,:])**2, axis = -1)] # each member list has a shape of (K,)\n    # find index of the closest centroid to each of the data point\n    closest_cluster = np.argmin(D, axis = 0) # cluster_class can take any value from [0, K-1]\n    rnk = np.eye(K)[closest_cluster] # construct one-hot -&gt; rnk: n x k\n\n  # Phase 2: for fix rnk update mu\n  for k in range(K):\n    data_k = X[rnk[:, k].astype(bool), :]\n    if data_k.size != 0:\n      updated_mu[k, :] = np.sum(data_k, axis=0) / np.sum(rnk[:, k], axis=0)\n    else:\n      updated_mu[k,:] = mu[k, :]\n  # criterion\n  if np.sum((mu - updated_mu)**2) &lt;= np.finfo(float).eps:\n    print(\"Convergence criterion met, exiting!\")\n    flag = True\n    return rnk, updated_mu, flag\n  return rnk, updated_mu, flag\n\nK = 3\nmax_itr = 10\n# initialize cluster centroids randomly from the given data points\nidx_K = np.random.randint(0, X.shape[0], K)\nmu = X[idx_K, :]\ninitial_mu = mu.copy()\nfor i in range(max_itr):\n  rnk, mu, flag = k_means_clustering(X, K, mu)\n  if flag:\n    break\nif i == max_itr - 1:\n  print('Mximum iterations reached!')\n\n\nConvergence criterion met, exiting!\n\n\n\n\nCode\n# @title\nfig, axs = plt.subplots(1, 2, figsize=(8, 3))\naxs[0].scatter(X[:, 0], X[:, 1], s=2, color='black', label=\"Data\")\naxs[0].scatter(initial_mu[:, 0], initial_mu[:, 1], s=80, color='g', marker='x', label=\"Initial centroids\", linewidths=2)\naxs[0].legend(fontsize='small')\naxs[0].set_title(\"Original Data\")\naxs[0].set_xlim(min_x, max_x)\nc = ['black', 'blue', 'y']\nk = np.arange(K)\nfor i, k in zip(c, k):\n  axs[1].scatter(X[rnk[:, k].astype(bool), 0], X[rnk[:,k].astype(bool), 1], color=i, s=2)\naxs[1].scatter(initial_mu[:, 0], initial_mu[:, 1], s=80, color='g', marker='x', label=\"Initial centroids\", linewidths=2)\naxs[1].scatter(mu[:, 0], mu[:, 1], s=80, color='red', marker='x', label='Final centroids', linewidths=2)\naxs[1].legend(fontsize='small')\naxs[1].set_title(\"Clusters After K-means Convergence\")\naxs[1].set_xlim(min_x, max_x)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/vae/vae-v3.html",
    "href": "posts/vae/vae-v3.html",
    "title": "Variational Autoencoder",
    "section": "",
    "text": "If you have a foundational understanding of probability theory and wish to explore the connection between modeling distributions through neural networks, Variational Autoencoders (VAEs) offer a compelling framework. VAEs are a class of generative models that enable the encoding of high-dimensional data into a lower-dimensional space while utilizing a probabilistic framework. Unlike standard autoencoders, the probabilistic nature of VAEs makes them more powerful for various real-world tasks. Encoding data into a lower-dimensional space is crucial for several reasons, especially when dealing with complex data types like images and audio. For instance, consider an image containing objects such as trees, birds, and a person, with much of the background remaining blank. Despite the image’s many pixels, the essential information resides in a lower-dimensional space (the pixels corresponding to the objects). By capturing only the key features, we can simplify the representation of the image. Furthermore, lower-dimensional data requires less storage and computational resources, accelerating processes like image classification or recognition. Similarly, consider an audio waveform that conveys not only spoken words but also information about the speaker’s accent, gender, and emotion. This nuanced information often exists in a lower-dimensional space that isn’t directly observable. Typically, such underlying unobserved information can be modeled by using the concept of latent variables from the probability theory. To understand latent variables simply, consider the concept of happiness. Happiness is not something we can measure directly; instead, we infer it through observable indicators such as smiling, laughter, or self-reported satisfaction on surveys. In this context, happiness serves as a latent variable. Thus, for many applications, inferring latent variables from observations is of great interest. One effective technique for this is known as variational inference, which has strong connections to VAEs."
  },
  {
    "objectID": "posts/vae/vae-v3.html#vae-training-and-the-reparametrization-trick",
    "href": "posts/vae/vae-v3.html#vae-training-and-the-reparametrization-trick",
    "title": "Variational Autoencoder",
    "section": "VAE training and the reparametrization trick",
    "text": "VAE training and the reparametrization trick\nWith this understanding of each of the terms involved in \\(ELBO\\), let’s now get back to it. We need to solve the following optimization problem during training of VAE: \\[\\begin{align*}\n\\underset{\\phi, \\theta}{\\mathrm{argmax}} ELBO(\\phi, \\theta) = \\underset{\\phi, \\theta}{\\mathrm{argmax}} E_{q_{\\phi}}[\\ln p(z) + \\ln p_{\\theta}(x|z) - \\ln q_{\\phi}(z|x)]\n\\end{align*}\\] This requires us to take the gradients of \\(ELBO(\\phi, \\theta)\\) with respect to the parameters \\(\\phi\\), and \\(\\theta\\). Let’s denote the gradient operator by \\(\\nabla\\). There is a hurdle for computing the gradient w.r.t. \\(\\phi\\). We can not take the gradient operator inside the expectation. This is because the expectation itself involves a distribution that is a function of \\(\\phi\\), mathematically speaking \\(\\nabla_{\\phi} E_{q_{\\phi}}\\ln q_{\\phi}(z|x) \\neq E_{q_{\\phi}} \\nabla_{\\phi} \\ln q_{\\phi}(z|x)\\). In contrast, such an issue does not arise for \\(\\nabla_{\\theta}\\). We use the reparametrization trick to overcome the hurdle. The idea is to make the expectation independent of \\(\\phi\\) by reparameterizing the distribution of \\(z\\). Consider \\[\\begin{align*}\nz &= g_{\\phi}(\\epsilon, x) = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon~\\mathrm{where}~\\epsilon \\sim p(\\epsilon) = \\mathcal{N}(\\epsilon|0, I) \\in \\mathbb{R}^{L-1}\n\\end{align*}\\] In other words, we sample another random variable \\(\\epsilon\\) outside the training process and apply a linear transformation to it to get the value of \\(z\\). We can then re-write \\(ELBO\\) as follows. \\[\\begin{align*}\nELBO(\\phi, \\theta) = E_{p(\\epsilon)}[\\ln p(g_{\\phi}(\\epsilon, x)) + \\ln p_{\\theta}(x|g_{\\phi}(\\epsilon, x)) - \\ln q_{\\phi}(g_{\\phi}(\\epsilon, x)|x)]\n\\end{align*}\\] We use Monte Carlo simulation for computing the expectation. \\[\\begin{align*}\nELBO(\\phi, \\theta) &\\approx \\frac{1}{K}\\sum_{k=1}^{K} [\\ln p(g_{\\phi}(\\epsilon^{(k)}, x)) + \\ln p_{\\theta}(x|g_{\\phi}(\\epsilon^{(k)}, x)) - \\ln q_{\\phi}(g_{\\phi}(\\epsilon^{(k)}, x)|x)]\\\\\n\\mathrm{where}~\\quad \\quad &\\epsilon^{(k)} \\sim \\mathcal{N}(\\epsilon|0, I);~ k \\in \\mathbb{Z}\n\\end{align*}\\] Typically, we use a batch of examples \\(\\{x^{(n)}\\}_{n=1}^N\\) during training. \\[\\begin{align*}\nELBO\\big(\\phi, \\theta; \\{x^{(n)}\\}_{n=1}^{N}\\big) &\\approx \\frac{1}{KN} \\sum_{n=1}^N \\sum_{k=1}^{K} \\bigg[\\ln p\\big(g_{\\phi}(\\epsilon^{(k)}, x^{(n)})\\big) + \\ln p_{\\theta}\\big(x^{(n)}|g_{\\phi}(\\epsilon^{(k)}, x^{(n)})\\big) - \\ln q_{\\phi}\\big(g_{\\phi}(\\epsilon^{(k)}, x^{(n)})|x^{(n)}\\big)\\bigg]\\\\\n\\mathrm{where}~\\quad \\quad &\\epsilon^{(k)} \\sim \\mathcal{N}(\\epsilon|0, I);~ k \\in \\mathbb{Z}\n\\end{align*}\\] This concludes our discussion on the mathematical concepts underlying Variational Autoencoders. As is customary in my blogs, I have included a Python code snippet to reinforce your understanding of these ideas. I strongly encourage you to examine the code line by line and relate it to the mathematics discussed. Additionally, reviewing Gaussian distributions will enhance your comprehension of the code. You may be curious about the implications of not using the reparameterization trick and instead sampling directly from a normal distribution with the mean and variance given by the encoder’s outputs. In this context, it’s important to note that the \\(ELBO\\) involves expectations over this distribution. Without the reparameterization trick, sampling a random variable from a distribution that depends on optimization variables introduces stochasticity during backpropagation. This stochasticity complicates the gradient estimation process, leading to a high variance of the gradients. As a result, training the VAE becomes unstable without the reparameterization trick. To observe this effect, you can set use_rep=False in the Python code and analyze the behavior of the loss function. From the resulting loss curves, you will likely infer that the model fails to converge effectively. In contrast, employing the reparameterization trick stabilizes training and allows for more reliable convergence."
  },
  {
    "objectID": "posts/vae/vae-v3.html#the-coding-example-and-key-focus-areas",
    "href": "posts/vae/vae-v3.html#the-coding-example-and-key-focus-areas",
    "title": "Variational Autoencoder",
    "section": "The Coding Example and Key Focus Areas",
    "text": "The Coding Example and Key Focus Areas\n\nVAE Model Implementation: This example demonstrates the VAE model built from scratch for the MNIST dataset.\nEducational Purpose: The code is purely for educational purposes. You should be able to relate it to the mathematics behind the probability distributions involved in the \\(ELBO\\).\nContinuous Data Focus: We focus on the continuous case of data where \\(x \\in \\mathbb{R}^D\\). If you are interested in studying the modeling of reconstruction error for categorical data, you can visit VAE.\nELBO Implementation: Pay attention to the implementation of the \\(ELBO\\).\nImage Synthesis: A few synthesized images are saved on disk if save_images is set to True in the configuration.\nHardware Compatibility: The code is general enough to be executed on a CPU or single/multiple GPUs.\n\n\n\nCode\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport os, sys\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}, n_gpus: {torch.cuda.device_count()}\")\n# access class objects as attributes\nclass AttributeDict(dict):\n    def __getattr__(self, attr):\n        return self[attr]\n    def __setattr__(self, attr, value):\n        self[attr] = value\n\nconfig = {\"lr\":1e-3, \"batch_size\":16, \"EPS\":1e-12, \"epochs\":50, 'loss_period':10, \"max_patience\":20, \"save_images\":False}\nconfig = AttributeDict(config)\n# MNIST dataset download\nclass DatasetMnist(Dataset):\n    def __init__(self, train=True, transforms=None):\n        super(DatasetMnist, self).__init__()\n        self.transforms = transforms\n        if train == True:\n            self.data = datasets.MNIST(root=\"./train\", train=train, download=True, transform=None)\n        else:\n            self.data = datasets.MNIST(root=\"./test\", train=train, download=True, transform=None)\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        image, label = self.data[idx]\n        if self.transforms:\n            image = self.transforms(image)\n        return image, label\ntransform = transforms.Compose([transforms.ToTensor()])\nfull_train_data = DatasetMnist(train=True, transforms=transform)\nfull_test_data  = DatasetMnist(train=False, transforms=transform)\nnp.random.seed(300)\nn_train = np.random.choice(len(full_train_data), 500)\nn_test = np.random.choice(len(full_test_data), 100)\ntrain_data = torch.utils.data.Subset(full_train_data, indices=n_train)\nval_data = torch.utils.data.Subset(full_test_data, indices=n_test)\ntrain_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\nvalid_loader = DataLoader(val_data, batch_size=config.batch_size, shuffle=False)\n# test_loader = DataLoader(test_data, batch_size=config.batch_size, shuffle=False)\nprint(f\"MNIST -&gt; Train: {len(train_data)}, Valid: {len(val_data)}\")\n\nx_sample, y_sample = next(iter(train_loader))\nD = x_sample.shape[2] * x_sample.shape[3] # input dim\nH = 512 # hidden units\nL = 16 # latent dim\nPI = torch.Tensor(np.asarray(np.pi))\nEPS = config.EPS\nencoder_net = nn.Sequential(nn.Linear(D, H), nn.ReLU(), nn.Linear(H, H), nn.ReLU(), nn.Linear(H, 2 * L))\ndecoder_net = nn.Sequential(nn.Linear(L, H), nn.ReLU(), nn.Linear(H, H), nn.ReLU(), nn.Linear(H, D))\n\ndef log_normal_diag(z, mu, logvar):\n    # z: [B, L]\n    log_p = -0.5 * z.shape[-1] * torch.log(2 * PI) - 0.5 * torch.sum(logvar, dim=-1) - 0.5 * torch.sum(((z - mu) ** 2) / torch.exp(logvar), dim=-1)\n    return log_p #[B]\ndef log_normal_standard(z):\n    # z: [B, L]\n    log_p = -0.5 * torch.log(2 * PI) - 0.5 * torch.sum(z**2, dim=-1)\n    return log_p #[B]\n    \nclass Prior(nn.Module):\n    def __init__(self):\n        super(Prior, self).__init__()\n    def sample(self):\n        pass\n    def log_prob(self, z):\n        log_p = log_normal_standard(z)\n        return log_p #[B]\n# VAE\nclass VAE(nn.Module):\n    def __init__(self, encoder_net, decoder_net, use_rep=True):\n        super(VAE, self).__init__()\n        self.encoder = encoder_net\n        self.decoder = decoder_net\n        self.prior   = Prior()\n        self.use_rep = use_rep\n    def sample_from_encoder(self, mu, logvar):\n        assert not logvar.isnan().any(), \"logvar -&gt; nan\"\n        assert not mu.isnan().any(), \"mu -&gt; nan\"\n        std = torch.exp(0.5 * logvar)\n        assert not std.isnan().any(), \"std -&gt; nan\"\n        if self.use_rep:\n            eps = torch.randn_like(std)\n            z = mu + std * eps\n        else:\n            z = torch.normal(mu, std)\n        assert not z.isnan().any(), \"z -&gt; nan in sample_from_dec\"\n        return z\n    def loss(self, x, enc_log_p, log_prior, dec_p):\n        # x --&gt; Encoder: q(z|x) --&gt; z --&gt; Decoder: p(z|x)\n        # ELBO = ln p(z) + Ez~q(z|x) ln p(z|x) - Ez~q(z|x) ln q(z|x) =  log_prior + dec_log_prob - enc_log_prob, x: 1 x D, z: 1 X L\n        # Assumptions for the distributions:  encoder: q(z|x) ~ Normal(z|mu(x), diag(var(x))), decoder: p(x|z), and p(z) ~ Normal(z;0, I)\n#         enc_log_p = log_normal_diag(z, mu, logvar) # ln q(z|x), [B]\n        dec_p = torch.clamp(dec_p, EPS, 1 - EPS)\n#         dec_log_p = torch.log() # ln p[x|z], clamp it to avoid log(0), [B, D]\n#         RE = F.cross_entropy(dec_p, x, reduction='none')\n#         RE = F.binary_cross_entropy(dec_p, x, reduction='none') # returns with -ve sign\n        RE = - (x * torch.log(dec_p) + (1 - x)* torch.log(1 - dec_p))\n        RE = torch.sum(RE, dim=-1) #[B], note that x ia d-dim vector, the joint distribution p[x|z] is written as the product of distributions for each dimension and hence the summation for log probability\n#         dec_log_p = torch.sum(dec_log_p, dim=-1) #[B], note that x ia d-dim vector, the joint distribution p[x|z] is written as the product of distributions for each dimension and hence the summation for log probability\n#         log_prior = self.prior.log_prob(z) # p[z], shape=[B]\n        KL = (log_prior - enc_log_p)\n        neg_elbo = RE - KL #[B]\n        return  torch.mean(neg_elbo) # batch-wise avg loss\n    def forward(self, x):\n        h_e = self.encoder(x)\n        mu, logvar = torch.chunk(h_e, 2, dim=-1) # mu, logvar: [B, L]\n        z = self.sample_from_encoder(mu, logvar) # [B, L], we use only one sample for Monte Carlo simulation\n        h_d = self.decoder(z)\n        dec_p = torch.sigmoid(h_d)\n#         dec_p = torch.softmax(self.decoder(z), dim=-1) # [B, D]\n        enc_log_p = log_normal_diag(z, mu, logvar)\n        log_prior = self.prior.log_prob(z) # p[z], shape=[B]\n#         loss = self.loss(z, mu, logvar, dec_p)\n        return mu, logvar, enc_log_p, log_prior, dec_p\n# training \nmodel = VAE(encoder_net, decoder_net, use_rep=True)\nif torch.cuda.device_count() &gt; 1:\n    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n    model = nn.DataParallel(model)\nmodel.to(device)\n# optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=config.lr)\noptimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n\ndef check(value):\n    assert not value.isnan().all(), f'{value} -&gt; nan'\n    assert value.isfinite().all(), f\"{value} -&gt; inf\"\n    \ndef train(train_loader, model, optimizer):\n    model.train()\n    avg_loss, total_samples = 0.0, 0\n    for x, y in train_loader:\n        batch_size = x.shape[0]\n        x = x.view(-1, D)\n        x = (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n        x = x.to(device)\n        mu, logvar, enc_log_p, log_prior, dec_p = model(x)\n        if torch.cuda.device_count() &gt; 1:\n            loss_ = model.module.loss(x, enc_log_p, log_prior, dec_p) # batch-wise avg loss\n        else:\n            loss_ = model.loss(x, enc_log_p, log_prior, dec_p) # batch-wise avg loss\n#         check(loss_)\n        optimizer.zero_grad()\n        loss_.backward()\n        optimizer.step()\n        avg_loss += (loss_.item() * batch_size)\n        total_samples += batch_size\n    avg_loss /= total_samples\n    return avg_loss\ndef save_images(org, recons, epoch):\n    \"\"\" org: [B, D]\n    \"\"\"\n    fig_name = f\"vae_output_{epoch}.png\"\n    save_here = \"./vae_results/\" + fig_name\n    num_images = org.shape[0]\n    assert num_images % 2 == 0, \"Can not save images, batch_size must be even!\"\n    org, recons = org[:num_images].view(-1, 28, 28), recons[:num_images].view(-1, 28, 28)\n    img = torch.cat((org, recons), 0)\n    img = img.detach().numpy()\n    fig, ax = plt.subplots(2, num_images)\n    for idx in range(2 * num_images):\n        ax[idx // num_images, idx % num_images].imshow(img[idx])\n        ax[idx // num_images, idx % num_images].axis(\"off\")\n    plt.savefig(save_here, bbox_inches='tight')\n    \ndef validate(valid_loader, model, epoch):\n    model.eval()\n    avg_loss, total_samples = 0.0, 0\n    with torch.no_grad():\n        for x, y in valid_loader:\n            batch_size = x.shape[0]\n            x = x.view(-1, D)\n            x = (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n            x = x.to(device)\n            mu, logvar, enc_log_p, log_prior, dec_p = model(x)\n            if torch.cuda.device_count() &gt; 1:\n                loss_ = model.module.loss(x, enc_log_p, log_prior, dec_p) # batch-wise avg loss\n            else:\n                loss_ = model.loss(x, enc_log_p, log_prior, dec_p) # batch-wise avg loss\n            \n            avg_loss += (loss_.item() * batch_size)\n            total_samples += batch_size\n    if config.save_images == True:\n        save_images(x, dec_p, epoch)\n    avg_loss /= total_samples\n    return avg_loss\ntrain_loss, val_loss = [], []\nfor epoch in range(config.epochs):\n    train_loss_ = train(train_loader, model, optimizer)\n    val_loss_   = validate(valid_loader, model, epoch)\n    train_loss.append(train_loss_)\n    val_loss.append(val_loss_)\n    if (epoch + 1) % config.loss_period == 0:\n        print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss_:0.4f}, Val Loss: {val_loss_:0.4f}\")\nplt.plot(train_loss, label='training')\nplt.plot(val_loss, label='validation')\nplt.legend()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JKD Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nVariational Autoencoder\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n10 min\n\n\n\n\n\n\n\nGumbel-Max Trick\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\n6 min\n\n\n\n\n\n\n\nK-Means Algorithm\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  }
]