<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jitendra K Dhiman">
<meta name="dcterms.date" content="2024-10-28">

<title>Variational Autoencoder – JKD Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">JKD Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html"> 
<span class="menu-text">Archive</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://jitendradhiman.github.io/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/jitendra-kumar-dhiman-phd-b02462a0/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Variational Autoencoder</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jitendra K Dhiman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 28, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#vae-training-and-the-reparametrization-trick" id="toc-vae-training-and-the-reparametrization-trick" class="nav-link active" data-scroll-target="#vae-training-and-the-reparametrization-trick">VAE training and the reparametrization trick</a></li>
  <li><a href="#the-coding-example-and-key-focus-areas" id="toc-the-coding-example-and-key-focus-areas" class="nav-link" data-scroll-target="#the-coding-example-and-key-focus-areas">The Coding Example and Key Focus Areas</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>If you have a foundational understanding of probability theory and wish to explore the connection between modeling distributions through neural networks, Variational Autoencoders (VAEs) offer a compelling framework for this. VAEs are a class of generative models that enable the encoding of high-dimensional data into a lower-dimensional space while utilizing a probabilistic framework. Unlike standard autoencoders, the probabilistic nature of VAEs makes them more powerful for various real-world tasks. Encoding data into a lower-dimensional space is crucial for several reasons, especially when dealing with complex data types like images and audio. For instance, consider an image containing objects such as trees, birds, and a person, with much of the background remaining blank. Despite the image’s many pixels, the essential information resides in a lower-dimensional space (the pixels corresponding to the objects). By capturing only the key features, we can simplify the representation of the image. Furthermore, lower-dimensional data requires less storage and computational resources, accelerating processes like image classification or recognition. Similarly, consider an audio waveform that conveys not only spoken words but also information about the speaker’s accent, gender, and emotion. This nuanced information often exists in a lower-dimensional space that isn’t directly observable. Typically, such an underlying unobserved information can be modelled by using the concept of <strong>latent variables</strong> from the probability theory. To understand latent variables simply, consider the concept of happiness. Happiness is not something we can measure directly; instead, we infer it through observable indicators such as smiling, laughter, or self-reported satisfaction on surveys. In this context, happiness serves as a latent variable. Thus, for many applications, inferring latent variables from observations is of great interest. One effective technique for this is known as variational inference, which has strong connections to VAEs. <!-- In an image classification task, a model might learn that certain pixel patterns correspond to a smile or the laughter.<br> --> ### Variational inference Lets denote an observable variable by <span class="math inline">\(x \in \mathbb{R}^D\)</span> or <span class="math inline">\(x \in \mathbb{Z}^D\)</span> and the latent variable by <span class="math inline">\(z \in \mathbb{R}^L\)</span>, where <span class="math inline">\(L &lt;&lt; D\)</span>. The VAE model is an encoder-decoder architecture, the encoder is responsible to encode the information into the latent variable space and the decoder aims to reconstruct the original information back from the latent representations. Under the paradigm of generative modelling, the VAEs can be viewed as latent variable model where we assume that the observable data <span class="math inline">\(x\)</span> generated from hidden latent variable <span class="math inline">\(z\)</span> and we would like to model the probablitiy distribution of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span>: <span class="math inline">\(p(z|x)\)</span>, also known as <em>posterior</em>. This allows us to infer the characteristics of <span class="math inline">\(z\)</span>. In other words, we would like to compute <span class="math display">\[\begin{align}
    p(z|x) = \frac{p(x|z)p(z)}{\int p(x, z)\mathrm{d}z}
\end{align}\]</span> However, directly computing this integral is intractable for many practical use-cases since the neumerator invloves summing over all the possibilities of latent variables. VAEs introduce a solution through <strong>variational inference</strong>. Instead of directly computing <span class="math inline">\(p(z|x)\)</span>, VAE approximates this posterior using a simpler distribution <span class="math inline">\(q(z|x)\)</span> making it as close as to the original distribution <span class="math inline">\(p(z|x)\)</span>.</p>
<p>To achieve this, one could minimize the KL divergence between these distributions of latent variable <span class="math inline">\(z\)</span>. <span class="math display">\[\begin{align}
KL(q(z|x)||p(z|x)) = E_{z \sim q(z|x)} \ln \frac{q(z|x)}{p(z|x)}
\end{align}\]</span> For simplicity we will write <span class="math inline">\(E_{z \sim q(z|x)}\)</span> as <span class="math inline">\(E_{q}\)</span>. The KL divergence can be re-written as follows. <span class="math display">\[\begin{align}
KL(q(z|x)||p(z|x)) &amp;= E_{q}\ln \frac{q(z|x)p(x)}{p(x|z)p(z)} = E_q \ln \frac{q(z|x)}{p(x|z)p(z)} + \ln p(x)
\end{align}\]</span> where we have used <span class="math inline">\(E_q \ln p(x) = \int \ln p(x) q(z|x)\mathrm{d}z = \ln p(x) \int q(z|x)\mathrm{d}z = \ln p(x) \cdot 1\)</span>. The second term in this equation does not depend on <span class="math inline">\(z\)</span> and using the fact that <span class="math inline">\(KL(\cdot||\cdot) \geq 0\)</span>, we can minimize only the first term which is also the lower bound on log-likelihood: <span class="math display">\[\begin{align}
\ln p(x) \geq - E_q \ln \frac{q(z|x)}{p(x|z)p(z)}.
\end{align}\]</span> In other words, we can maximize the RHS in this inequality. This quantity is known as Evidence Lower Bound (<span class="math inline">\(ELBO\)</span>). Re-writing <span class="math inline">\(ELBO\)</span>: <span class="math display">\[\begin{align}
ELBO &amp;= - E_q \ln \frac{q(z|x)}{p(x|z)p(z)} = E_q \ln p(z) + E_q \ln p(x|z) - E_q \ln q(z|x)
\end{align}\]</span> The first term is log-prior, the second term represents the faithfullness of the decoder-reconstruction from latent variable <span class="math inline">\(z\)</span> back to original data <span class="math inline">\(x\)</span>, the third term is the distribution over <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span>. In VAE, the distributions q(z|x) and p(x|z) are parameterized by using endoder and decoder networks, respectively. You can think of this pipeline <span class="math inline">\(x \overset{q_{\phi}(z|x)}{\longrightarrow} z \overset{p_{\theta}(x|z)}{\longrightarrow} \tilde x\)</span>, it would help you to keep the overall picture in mind. Here <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\theta\)</span> represent the parameters of the encoder and decoder, respectively. You are free to choose any neural network architecture for these two blocks of VAE. Let’s discuss each of the distributions invloved in <span class="math inline">\(ELBO\)</span>. ### VAE encoder We would like to choose <span class="math inline">\(q_{\phi}(z|x)\)</span> to be mathematically tractable (afterall that is why we introduced it). Typically, it is chosen to be a Gaussian. <span class="math display">\[\begin{align}
q_{\phi}(z|x) \sim \mathcal{N}(z|\mu_{\phi}(x), \sigma_{\phi}^2(x)I);~ z \in \mathbb{R}^{L}, x \in \mathbb{R}^{D}
\end{align}\]</span> where the mean <span class="math inline">\(\mu_{\phi}(x)\)</span> and the variance <span class="math inline">\(\sigma_{\phi}^2(x)\)</span> are the outputs of encoder, and we have chosen the covariance matrix to be diagonal (<span class="math inline">\(I\)</span> is an identity matrix) without the loss of generality. Since variance has to be non-negative, during implementation you can assume that encoder ouputs <span class="math inline">\(\log\)</span> variance <span class="math inline">\(logvar\)</span> and use <span class="math inline">\(\exp(\log {(logvar)})\)</span> to get the variance on the linear scale if required, this trick avoids variance being negative. ### Prior The prior <span class="math inline">\(p(z)\)</span> plays a crucial role in VAE. For our discussion, we model it using standard Gaussian (with mean 0 and variance 1), <span class="math display">\[\begin{align*}
p(z) = \mathcal{N}(z|0, I)
\end{align*}\]</span> where <span class="math inline">\(I\)</span> denots an identity matrix of size <span class="math inline">\(L \times L\)</span> since <span class="math inline">\(z \in \mathbb{R}^L\)</span>. ### VAE decoder The term <span class="math inline">\(p_{\theta}(x|z)\)</span> represents decoder likelihood for reconstructing <span class="math inline">\(x\)</span> from <span class="math inline">\(z\)</span> and hence it is also referred to as the reconstruction error. The form of this term depends on the type of random variable <span class="math inline">\(x\)</span>. - For categorical <span class="math inline">\(x\)</span>, we can model <span class="math inline">\(p_{\theta}(x|z)\)</span> as categorical distribution parmeterized by the decoder probablities. - For continuous <span class="math inline">\(x\)</span>, we can use mean square error or binary/cross entropy as recnstruction error between the values of <span class="math inline">\(x\)</span> and the predcited values from decoder.<br></p>
<p>To understand further, let’s disscuss these formulations of <span class="math inline">\(p_{\theta}(x|z)\)</span> for image generation task. #### Formulation-1 Under this formulation, we assume that <span class="math inline">\(x\)</span> is a categorical random vector <span class="math inline">\(x = [x_1, x_2, \dots, x_d, \dots, x_D], x_d \in [0, 1, \dots, L-1]\)</span>. For instance, you can think of an image <span class="math inline">\(x\)</span> of size <span class="math inline">\(8 \times 8\)</span> where each pixel can take a value in the range from <span class="math inline">\(0\)</span> to <span class="math inline">\(16\)</span> i.e.&nbsp;<span class="math inline">\(x_d \in [0, 1, \dots, 16]^{64}\)</span>, in this case we have <span class="math inline">\(L=17, D=64\)</span>. The probability distribution of <span class="math inline">\(x_d\)</span> is given by <span class="math display">\[\begin{align*}
    p(x_d)  = \prod_{l=1}^{L-1} p_{dl}^{I_{\{x_d = l\}}},~0 \leq l \leq L - 1
\end{align*}\]</span> where <span class="math display">\[\begin{align}
I(x) =
\begin{cases}
1 &amp; \text{if } x_d = l \\
0 &amp; \text{if } x_d \neq l
\end{cases}
\end{align}\]</span> is an indicator variable and <span class="math inline">\(p_{dl} = P(x_d=l)\)</span>. Assuming that <span class="math inline">\(x_d\)</span>’s are i.i.d., then <span class="math display">\[\begin{align*}
p_{\theta}(x|z) = \prod_{d=1}^{D}\prod_{l=1}^{L-1} p_{dl}^{I_{\{x_d = l\}}}
\end{align*}\]</span> In this equation the probabilities <span class="math inline">\(p_{d} = [p_{d0}, \dots, p_{dl}, \dots, p_{dL-1}] \in \mathbb{R}^{L-1}\)</span> are given by the output layer of the decoder <em>i.e.</em> <span class="math inline">\(p_d = \mathrm{softmax}(f_{\theta}(z))\)</span> with <span class="math inline">\(f_{\theta}(z) \in \mathbb{R}^{L-1}\)</span> and <span class="math inline">\(f_{\theta}(\cdot)\)</span> representing the decoder. #### Formulation-2 In formulation-1, we derived the reconstruction error <span class="math inline">\(p_{\theta}(x|z)\)</span> for the discrete case <strong>i.e.</strong> <span class="math inline">\(x_d \in \mathbb{Z}^D\)</span>. In this formulation, we seek the treatment for the continuous case <strong>i.e.</strong> <span class="math inline">\(x_d \in \mathbb{R}^D\)</span>. Further, we assume that <span class="math inline">\(x_d\)</span> is normalized <strong>i.e.</strong> <span class="math inline">\(x_d \in [0, 1]\)</span>. An approperiate choice is the binary-cross entropy with soft labels given by <span class="math inline">\(x_d\)</span>. <span class="math display">\[\begin{align*}
\ln p_{\theta}(x|z) = - \sum_{d=1}^{D} x_d \ln \hat p_d(z)
\end{align*}\]</span> where <span class="math inline">\(\hat p_d(z) \in \mathbb{R}^{L-1}\)</span> are given by the decoder that is <span class="math inline">\(\hat p_d(z) = \mathrm{sigmoid}(f_{\theta}(z))\)</span>. We use <span class="math inline">\(\mathrm{sigmoid}(\cdot)\)</span> to squash each pixel value between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. ### Summary - <span class="math inline">\(ELBO = E_q \ln p(z) + E_q \ln p_{\theta}(x|z) - E_q \ln q_{\phi}(z|x)\)</span> - <span class="math inline">\(p(z) \sim \mathcal{N}(z|0, I)\)</span> - <span class="math inline">\(p_{\theta}(x|z)\)</span> is the reconstruction error and depends on <span class="math inline">\(x\)</span> - <span class="math inline">\(q_{\phi}(z|x) \sim \mathcal{N}(z|\mu_{\phi}(x), \sigma^2_{\phi}(x)I)\)</span> is known as the variational posterior</p>
<section id="vae-training-and-the-reparametrization-trick" class="level2">
<h2 class="anchored" data-anchor-id="vae-training-and-the-reparametrization-trick">VAE training and the reparametrization trick</h2>
<p>With this understanding of each of the terms involved in <span class="math inline">\(ELBO\)</span>, let’s now get back to it. We need to solve the following optimization problem during training of VAE: <span class="math display">\[\begin{align*}
\underset{\phi, \theta}{\mathrm{argmax}} ELBO(\phi, \theta) = \underset{\phi, \theta}{\mathrm{argmax}} E_{q_{\phi}}[\ln p(z) + \ln p_{\theta}(x|z) - \ln q_{\phi}(z|x)]
\end{align*}\]</span> This requires us to take the gradients of <span class="math inline">\(ELBO(\phi, \theta)\)</span> with respect to the parameters <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(\theta\)</span>. Let’s denote the gradient operator by <span class="math inline">\(\nabla\)</span>. There is a hurdle for computing the gradient w.r.t. <span class="math inline">\(\phi\)</span>. We can not take the gradient operator inside the expectation. This is because the expectation itself involves a distribution that is a function of <span class="math inline">\(\phi\)</span>, mathematically speaking <span class="math inline">\(\nabla_{\phi} E_{q_{\phi}}\ln _{\phi}(z|x) \neq E_{q_{\phi}} \nabla_{\phi} \ln _{\phi}(z|x)\)</span>. In contrast, such issue does not arise for <span class="math inline">\(\nabla_{\theta}\)</span>. We use reparametrization trick to overcome the hurdle. The idea is to make the expectation independent of <span class="math inline">\(\phi\)</span> by reparameterizing the distribution of <span class="math inline">\(z\)</span>. Consider <span class="math display">\[\begin{align*}
z &amp;= g_{\phi}(\epsilon, x) = \mu_{\phi}(x) + \sigma_{\phi}(x) \odot \epsilon~\mathrm{where}~\epsilon \sim p(\epsilon) = \mathcal{N}(\epsilon|0, I) \in \mathbb{R}^{L-1}
\end{align*}\]</span> In other words, we sample another random variable <span class="math inline">\(\epsilon\)</span> outside the training process and apply a linear transformation on it to get the value of <span class="math inline">\(z\)</span>. We can then re-write <span class="math inline">\(ELBO\)</span> as follows. <span class="math display">\[\begin{align*}
ELBO(\phi, \theta) = E_{p(\epsilon)}[\ln p(g_{\phi}(\epsilon, x)) + \ln p_{\theta}(x|g_{\phi}(\epsilon, x)) - \ln q_{\phi}(g_{\phi}(\epsilon, x)|x)]
\end{align*}\]</span> We use <strong>Monte-carlo simulation</strong> for computing the expectation. <span class="math display">\[\begin{align*}
ELBO(\phi, \theta) &amp;\approx \frac{1}{K}\sum_{k=1}^{K} [\ln p(g_{\phi}(\epsilon^{(k)}, x)) + \ln p_{\theta}(x|g_{\phi}(\epsilon^{(k)}, x)) - \ln q_{\phi}(g_{\phi}(\epsilon^{(k)}, x)|x)]\\
\mathrm{where}~\quad \quad &amp;\epsilon^{(k)} \sim \mathcal{N}(\epsilon|0, I);~ k \in \mathbb{Z}
\end{align*}\]</span> Typically, we use a batch of examples <span class="math inline">\(\{x^{(n)}\}_{n=1}^N\)</span> during training. <span class="math display">\[\begin{align*}
ELBO\big(\phi, \theta; \{x^{(n)}\}_{n=1}^{N}\big) &amp;\approx \frac{1}{KN} \sum_{n=1}^N \sum_{k=1}^{K} \bigg[\ln p\big(g_{\phi}(\epsilon^{(k)}, x^{(n)})\big) + \ln p_{\theta}\big(x^{(n)}|g_{\phi}(\epsilon^{(k)}, x^{(n)})\big) - \ln q_{\phi}\big(g_{\phi}(\epsilon^{(k)}, x^{(n)})|x^{(n)}\big)\bigg]\\
\mathrm{where}~\quad \quad &amp;\epsilon^{(k)} \sim \mathcal{N}(\epsilon|0, I);~ k \in \mathbb{Z}
\end{align*}\]</span> This concludes our discussion on the mathematical concepts underlying Variational Autoencoders. As is customary in my blogs, I have included a Python code snippet to reinforce your understanding of these ideas. I strongly encourage you to examine the code line by line and relate it back to the mathematics discussed. Additionally, reviewing Gaussian distributions will enhance your comprehension of the code.<br> You may be curious about the implications of not using the reparameterization trick and instead sampling directly from a normal distribution with the mean and variance given by the encoder’s outputs. In this context, it’s important to note that the <span class="math inline">\(ELBO\)</span> involves expectations over this distribution. Without the reparameterization trick, sampling a random variable from a distribution that depends on optimization variables introduces stochasticity during backpropagation. This stochasticity complicates the gradient estimation process, leading to high variance of the gradients. As a result, training the VAE becomes unstable without the reparameterization trick. To observe this effect, you can set <code>use_rep=False</code> in the Python code and analyze the behavior of the loss function. From the resulting loss curves, you will likely infer that the model fails to converge effectively. In contrast, employing the reparameterization trick stabilizes training and allows for more reliable convergence.</p>
</section>
<section id="the-coding-example-and-key-focus-areas" class="level2">
<h2 class="anchored" data-anchor-id="the-coding-example-and-key-focus-areas">The Coding Example and Key Focus Areas</h2>
<ul>
<li><strong>VAE Model Implementation</strong>: This example demonstrates the VAE model built from scratch for the MNIST dataset.<br></li>
<li><strong>Educational Purpose</strong>: The code is purely for educational purposes. You should be able to relate it to the mathematics behind the probability distributions involved in the <span class="math inline">\(ELBO\)</span>.<br></li>
<li><strong>Continuous Data Focus</strong>: We focus on the continuous case of data where <span class="math inline">\(x \in \mathbb{R}^D\)</span>. If you are interested in studying the modeling of reconstruction error for categorical data, you can visit <a href="https://github.com/Jitendradhiman/SaralSpeech/blob/master/vae_v2.py">VAE</a>.<br></li>
<li><strong>ELBO Implementation</strong>: Pay attention to the implementation of the <span class="math inline">\(ELBO\)</span>.<br></li>
<li><strong>Image Synthesis</strong>: A few synthesized images are saved on disk if <code>save_images</code> is set to True in the configuration.<br></li>
<li><strong>Hardware Compatibility</strong>: The code is general enough to be executed on a CPU or single/multiple GPUs.<br></li>
</ul>
<div id="42d7f6ad" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-10-28T05:04:02.320574Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-10-28T05:04:02.319534Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-10-28T05:04:07.580388Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-10-28T05:04:07.578973Z&quot;}" data-papermill="{&quot;duration&quot;:5.2695,&quot;end_time&quot;:&quot;2024-10-28T05:04:07.583059&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2024-10-28T05:04:02.313559&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, sys</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">, n_gpus: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>device_count()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># access class objects as attributes</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttributeDict(<span class="bu">dict</span>):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getattr__</span>(<span class="va">self</span>, attr):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>[attr]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__setattr__</span>(<span class="va">self</span>, attr, value):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>[attr] <span class="op">=</span> value</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> {<span class="st">"lr"</span>:<span class="fl">1e-3</span>, <span class="st">"batch_size"</span>:<span class="dv">16</span>, <span class="st">"EPS"</span>:<span class="fl">1e-12</span>, <span class="st">"epochs"</span>:<span class="dv">50</span>, <span class="st">'loss_period'</span>:<span class="dv">10</span>, <span class="st">"max_patience"</span>:<span class="dv">20</span>, <span class="st">"save_images"</span>:<span class="va">False</span>}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> AttributeDict(config)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># MNIST dataset download</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DatasetMnist(Dataset):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train<span class="op">=</span><span class="va">True</span>, transforms<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(DatasetMnist, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transforms <span class="op">=</span> transforms</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> train <span class="op">==</span> <span class="va">True</span>:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.data <span class="op">=</span> datasets.MNIST(root<span class="op">=</span><span class="st">"./train"</span>, train<span class="op">=</span>train, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.data <span class="op">=</span> datasets.MNIST(root<span class="op">=</span><span class="st">"./test"</span>, train<span class="op">=</span>train, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        image, label <span class="op">=</span> <span class="va">self</span>.data[idx]</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transforms:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> <span class="va">self</span>.transforms(image)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([transforms.ToTensor()])</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>full_train_data <span class="op">=</span> DatasetMnist(train<span class="op">=</span><span class="va">True</span>, transforms<span class="op">=</span>transform)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>full_test_data  <span class="op">=</span> DatasetMnist(train<span class="op">=</span><span class="va">False</span>, transforms<span class="op">=</span>transform)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">300</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>n_train <span class="op">=</span> np.random.choice(<span class="bu">len</span>(full_train_data), <span class="dv">500</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>n_test <span class="op">=</span> np.random.choice(<span class="bu">len</span>(full_test_data), <span class="dv">100</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> torch.utils.data.Subset(full_train_data, indices<span class="op">=</span>n_train)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>val_data <span class="op">=</span> torch.utils.data.Subset(full_test_data, indices<span class="op">=</span>n_test)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_data, batch_size<span class="op">=</span>config.batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="op">=</span> DataLoader(val_data, batch_size<span class="op">=</span>config.batch_size, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co"># test_loader = DataLoader(test_data, batch_size=config.batch_size, shuffle=False)</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MNIST -&gt; Train: </span><span class="sc">{</span><span class="bu">len</span>(train_data)<span class="sc">}</span><span class="ss">, Valid: </span><span class="sc">{</span><span class="bu">len</span>(val_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>x_sample, y_sample <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> x_sample.shape[<span class="dv">2</span>] <span class="op">*</span> x_sample.shape[<span class="dv">3</span>] <span class="co"># input dim</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> <span class="dv">512</span> <span class="co"># hidden units</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="dv">16</span> <span class="co"># latent dim</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>PI <span class="op">=</span> torch.Tensor(np.asarray(np.pi))</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>EPS <span class="op">=</span> config.EPS</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>encoder_net <span class="op">=</span> nn.Sequential(nn.Linear(D, H), nn.ReLU(), nn.Linear(H, H), nn.ReLU(), nn.Linear(H, <span class="dv">2</span> <span class="op">*</span> L))</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>decoder_net <span class="op">=</span> nn.Sequential(nn.Linear(L, H), nn.ReLU(), nn.Linear(H, H), nn.ReLU(), nn.Linear(H, D))</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_normal_diag(z, mu, logvar):</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># z: [B, L]</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    log_p <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> z.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">*</span> torch.log(<span class="dv">2</span> <span class="op">*</span> PI) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(logvar, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(((z <span class="op">-</span> mu) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> torch.exp(logvar), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_p <span class="co">#[B]</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_normal_standard(z):</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># z: [B, L]</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    log_p <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.log(<span class="dv">2</span> <span class="op">*</span> PI) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(z<span class="op">**</span><span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_p <span class="co">#[B]</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Prior(nn.Module):</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Prior, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>):</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, z):</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>        log_p <span class="op">=</span> log_normal_standard(z)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> log_p <span class="co">#[B]</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="co"># VAE</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VAE(nn.Module):</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder_net, decoder_net, use_rep<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(VAE, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder_net</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder_net</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior   <span class="op">=</span> Prior()</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_rep <span class="op">=</span> use_rep</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_from_encoder(<span class="va">self</span>, mu, logvar):</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="kw">not</span> logvar.isnan().<span class="bu">any</span>(), <span class="st">"logvar -&gt; nan"</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="kw">not</span> mu.isnan().<span class="bu">any</span>(), <span class="st">"mu -&gt; nan"</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> logvar)</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="kw">not</span> std.isnan().<span class="bu">any</span>(), <span class="st">"std -&gt; nan"</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_rep:</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>            eps <span class="op">=</span> torch.randn_like(std)</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> mu <span class="op">+</span> std <span class="op">*</span> eps</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.normal(mu, std)</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="kw">not</span> z.isnan().<span class="bu">any</span>(), <span class="st">"z -&gt; nan in sample_from_dec"</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, enc_log_p, log_prior, dec_p):</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x --&gt; Encoder: q(z|x) --&gt; z --&gt; Decoder: p(z|x)</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ELBO = ln p(z) + Ez~q(z|x) ln p(z|x) - Ez~q(z|x) ln q(z|x) +  =  log_prior + dec_log_prob - enc_log_prob, x: 1 x D, z: 1 X L</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assumptions for the distributions:  encoder: q(z|x) ~ Normal(z|mu(x), diag(var(x))), decoder: p(x|z), and p(z) ~ Normal(z;0, I)</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="co">#         enc_log_p = log_normal_diag(z, mu, logvar) # ln q(z|x), [B]</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>        dec_p <span class="op">=</span> torch.clamp(dec_p, EPS, <span class="dv">1</span> <span class="op">-</span> EPS)</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="co">#         dec_log_p = torch.log() # ln p[x|z], clamp it to avoid log(0), [B, D]</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="co">#         RE = F.cross_entropy(dec_p, x, reduction='none')</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="co">#         RE = F.binary_cross_entropy(dec_p, x, reduction='none') # returns with -ve sign</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>        RE <span class="op">=</span> <span class="op">-</span> (x <span class="op">*</span> torch.log(dec_p) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> x)<span class="op">*</span> torch.log(<span class="dv">1</span> <span class="op">-</span> dec_p))</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>        RE <span class="op">=</span> torch.<span class="bu">sum</span>(RE, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co">#[B], note that x ia d-dim vector, the joint distribution p[x|z] is written as the product of distributions for each dimension and hence the summation for log probability</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="co">#         dec_log_p = torch.sum(dec_log_p, dim=-1) #[B], note that x ia d-dim vector, the joint distribution p[x|z] is written as the product of distributions for each dimension and hence the summation for log probability</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="co">#         log_prior = self.prior.log_prob(z) # p[z], shape=[B]</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>        KL <span class="op">=</span> (log_prior <span class="op">-</span> enc_log_p)</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>        neg_elbo <span class="op">=</span> RE <span class="op">-</span> KL <span class="co">#[B]</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>  torch.mean(neg_elbo) <span class="co"># batch-wise avg loss</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>        h_e <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>        mu, logvar <span class="op">=</span> torch.chunk(h_e, <span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># mu, logvar: [B, L]</span></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.sample_from_encoder(mu, logvar) <span class="co"># [B, L], we use only one sample for Monte Carlo simulation</span></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>        h_d <span class="op">=</span> <span class="va">self</span>.decoder(z)</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>        dec_p <span class="op">=</span> torch.sigmoid(h_d)</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="co">#         dec_p = torch.softmax(self.decoder(z), dim=-1) # [B, D]</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>        enc_log_p <span class="op">=</span> log_normal_diag(z, mu, logvar)</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>        log_prior <span class="op">=</span> <span class="va">self</span>.prior.log_prob(z) <span class="co"># p[z], shape=[B]</span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="co">#         loss = self.loss(z, mu, logvar, dec_p)</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu, logvar, enc_log_p, log_prior, dec_p</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="co"># training </span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VAE(encoder_net, decoder_net, use_rep<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.device_count() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Available GPUs: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>device_count()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> nn.DataParallel(model)</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=config.lr)</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>config.lr)</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check(value):</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> value.isnan().<span class="bu">all</span>(), <span class="ss">f'</span><span class="sc">{</span>value<span class="sc">}</span><span class="ss"> -&gt; nan'</span></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> value.isfinite().<span class="bu">all</span>(), <span class="ss">f"</span><span class="sc">{</span>value<span class="sc">}</span><span class="ss"> -&gt; inf"</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(train_loader, model, optimizer):</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>    avg_loss, total_samples <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> train_loader:</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, D)</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> (x <span class="op">-</span> torch.<span class="bu">min</span>(x)) <span class="op">/</span> (torch.<span class="bu">max</span>(x) <span class="op">-</span> torch.<span class="bu">min</span>(x))</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(device)</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>        mu, logvar, enc_log_p, log_prior, dec_p <span class="op">=</span> model(x)</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.cuda.device_count() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>            loss_ <span class="op">=</span> model.module.loss(x, enc_log_p, log_prior, dec_p) <span class="co"># batch-wise avg loss</span></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>            loss_ <span class="op">=</span> model.loss(x, enc_log_p, log_prior, dec_p) <span class="co"># batch-wise avg loss</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="co">#         check(loss_)</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>        loss_.backward()</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>        avg_loss <span class="op">+=</span> (loss_.item() <span class="op">*</span> batch_size)</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>        total_samples <span class="op">+=</span> batch_size</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">/=</span> total_samples</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_loss</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_images(org, recons, epoch):</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" org: [B, D]</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>    fig_name <span class="op">=</span> <span class="ss">f"vae_output_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png"</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>    save_here <span class="op">=</span> <span class="st">"./vae_results/"</span> <span class="op">+</span> fig_name</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>    num_images <span class="op">=</span> org.shape[<span class="dv">0</span>]</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> num_images <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>, <span class="st">"Can not save images, batch_size must be even!"</span></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>    org, recons <span class="op">=</span> org[:num_images].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>), recons[:num_images].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> torch.cat((org, recons), <span class="dv">0</span>)</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> img.detach().numpy()</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, num_images)</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span> <span class="op">*</span> num_images):</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>        ax[idx <span class="op">//</span> num_images, idx <span class="op">%</span> num_images].imshow(img[idx])</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>        ax[idx <span class="op">//</span> num_images, idx <span class="op">%</span> num_images].axis(<span class="st">"off"</span>)</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_here, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validate(valid_loader, model, epoch):</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>    avg_loss, total_samples <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> valid_loader:</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>            batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, D)</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> (x <span class="op">-</span> torch.<span class="bu">min</span>(x)) <span class="op">/</span> (torch.<span class="bu">max</span>(x) <span class="op">-</span> torch.<span class="bu">min</span>(x))</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.to(device)</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>            mu, logvar, enc_log_p, log_prior, dec_p <span class="op">=</span> model(x)</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> torch.cuda.device_count() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>                loss_ <span class="op">=</span> model.module.loss(x, enc_log_p, log_prior, dec_p) <span class="co"># batch-wise avg loss</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>                loss_ <span class="op">=</span> model.loss(x, enc_log_p, log_prior, dec_p) <span class="co"># batch-wise avg loss</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>            avg_loss <span class="op">+=</span> (loss_.item() <span class="op">*</span> batch_size)</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>            total_samples <span class="op">+=</span> batch_size</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> config.save_images <span class="op">==</span> <span class="va">True</span>:</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>        save_images(x, dec_p, epoch)</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">/=</span> total_samples</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_loss</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>train_loss, val_loss <span class="op">=</span> [], []</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(config.epochs):</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>    train_loss_ <span class="op">=</span> train(train_loader, model, optimizer)</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>    val_loss_   <span class="op">=</span> validate(valid_loader, model, epoch)</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>    train_loss.append(train_loss_)</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>    val_loss.append(val_loss_)</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> config.loss_period <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>train_loss_<span class="sc">:0.4f}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>val_loss_<span class="sc">:0.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>plt.plot(train_loss, label<span class="op">=</span><span class="st">'training'</span>)</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>plt.plot(val_loss, label<span class="op">=</span><span class="st">'validation'</span>)</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>