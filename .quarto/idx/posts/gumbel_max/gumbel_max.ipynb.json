{"title":"Gumbel-Max Trick","markdown":{"yaml":{"title":"Gumbel-Max Trick"},"headingText":"Inverse CDF method","containsRefs":false,"markdown":"\n\n\n\n> Pre-requisite: Probability theory\n\nThe Gumbel-max trick is useful technique for sampling from a discrete distribution, especially when built-in statistical functions are unavailable. Although many statistical software packages can handle a wide range of distributions, there are situations where you need to sample from distributions that may not be explicitly supported.\n\nIn this blog, we will discuss the Gumbel-max trick, it's mathematical proof, and a provide simulation example. Before diving into the trick, let's do something easy first. We will briefly revist the inverse cumulative distribution function (CDF) method for smampling form a distribution, which applies to both continuous and discrete distributions. In contrast, the Gumbel-max trick is specifically applicable to the discrete distributions.\n\n**Definition**: The CDF of a random variable $Z$ is given by $F_Z(z) = P(Z \\leq z)~\\forall~z$ where $P(\\cdot)$ denotes the probability. The CDF is a non-decreasing function defined on the real line $\\mathbb{R}$.<br>\n**Definition**: For a non-decreasing function $F$ on $\\mathbb{R}$, the generalized inverse is defined as $F^{-1}(u) = min\\{z: F(z) \\geq u\\}, u \\in \\mathbb{R}$. For a continuous random variable the inequality \"$\\geq$\" become equality \"$=$\".<br>\nSuppose that we want to sample $Z \\sim P(z)$, and we have access to inverse CDF of $Z$, then<br>\n**Lemma**:<br>\nIf $U \\sim \\text{Uniform}(0, 1)$, then $Z = F_Z^{-1}(U)$ is a simulation from $P(z)$.<br>\n***Proof***: Consider $P(Z \\leq z) = P(F_Z^{-1}(U) \\leq z) = P(U \\leq F_Z(z)) = \\int_{0}^{F_Z(z)} 1~\\text{d}u =  F_Z(z)$. Hence, $Z$ follows the desired distribution $P(z)$.\nWe can observe that the inverse CDF method can be used to generate samples of $Z$ through the transformation of a uniform random variable where the transformation is the inverse CDF of $Z$ itself. For most of the statistical softwares it is easier to generate samples from a uniform distribution.\n\n#### A Coding example\nSuppose $Z$ is a categorical random variable that is $Z$ can take values from $\\{1, 2, \\dots, K\\}$ with probabilities $P(Z=k)=p_k$ where $K$ denotes the categories. We would like to generate various realizations (samples) of $Z$ (e.g. $\\{Z_1, Z_2, \\cdots, Z_n\\}$) from this distribution. \nWe can employ the inverse CDF method. Please refer to the Python code example for more clarity, come back, and read again.<br>\n \n### Gumbel-max trick\n**Definition**: The Gumbel distribution of a random variable $X \\in \\mathbb{R}$ is given by $f_X(x) = \\frac{1}{\\beta}\\exp(-(x - \\mu) - \\exp(-(x - \\mu)/\\beta))$, the CDF is $F_X(x) = P(X \\leq x) = \\exp(-\\exp(-(x - \\mu)/\\beta))$, and the inverse CDF is $-\\log(\\log(x - \\mu)/\\beta)$. To the scope of this discussion, it is enough to consider $\\mu=0, \\beta=1$, that is $X \\sim \\text{Gumbel}(0, 1)$.<br>\nThe Gumbel-max trick converts the sampling problem into an optimization problem and the key result is as follows.<br>\n\\begin{align}\n\\tag{1}\nI = \\underset{k}{\\text{argmax}}~(\\log(p_k) + G_k) \\sim \\text{Categorical}(p_k), 1 \\leq k \\leq K\n\\end{align}\nwhere $G_k \\sim \\text{Gumbel}(0, 1)$ are Gumbel-distributed i.i.d random variables, and $I\\in \\mathbb{Z}$ is a random variable because of $G_k$. Eq. (1) states that the index $I$ follows the desired categorical distribution (see proof), this index is obtained by adding samples from the Gumbel distribution (the Gumbel noise) to the log probabilities and choosing the index for which the quantity is maximum. <br>\n***Proof***: Let $X_k = \\log(p_k) + G_k \\in \\mathbb{R}$. If the solution of Eq. (1) is the index $k$, then the event $\\{I = k\\}$ is equivalent to the event $\\{X_k > X_j~\\forall j \\neq k\\}$ in the probability space. We can further translate it to another equivalent event \"$\\{\\text{sum over all}~x~(X_k = x, X_j < x~\\forall j \\neq k)\\}$\". Please note that $G_k$'s are i.i.d and consequently $X_k$'s are also i.i.d.\nHence in probability space, we have<br>\n\\begin{align}\nP(I = k) &= P(X_k > X_j~\\forall k \\neq j) = \\int P(X_k = x, X_j < x~\\forall j \\neq k)~\\mathrm{d}x \\\\ \\nonumber\n&=\\int P(X_k = x) P(X_j < x~\\forall~j \\neq k|X_k = x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(X_k = x) P(X_j < x~\\forall~j \\neq k)~\\mathrm{d}x =  \\int P(X_k = x) \\prod_{j \\neq k} P(X_j < x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(G_k = x - log(p_k)) \\prod_{j \\neq k} P(G_j < x - \\log(p_j))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k) - x - \\exp(\\log(p_k) - x)) \\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k))\\exp(-x)\\exp(-\\exp(\\log(p_k) - x))\\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x)\\prod_{k} \\exp(-\\exp(\\log(p_k) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x) \\prod_{k} \\exp(-p_k\\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int \\exp(-x) \\exp(-\\sum_{k} p_k \\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int_{-\\infty}^{+\\infty} \\exp(-x) \\exp(-1\\exp(-x)))~\\mathrm{d}x = p_k\n\\end{align}\nThis completes the proof. Hence the random variable $I \\sim \\text{Cat}(p_k)$. To generate samples using this method, you only need to sample Gumbel-distributed variables $G_k$ and find the index that maximizes $\\log(p_k) + G_k$. In contrast to inverse CDF method, this avoids computing the CDF or performing a binary search.\nTo sample Gumbel-distributed variables, you can start with a uniform random variable and transform it using the inverse Gumbel CDF  $(-\\log(-\\log(-x)))$ which is a closed-form expression, the resulting random variable will follow the Gumbel distribution.\nWe can now generate various realizations of $I$, i.e. $\\{I_1, I_2, \\dots, I_n\\}$ by adding each time a different realization of Gumbel noise to the log-probabilities and picking up on the maximum index. Please refer to the Python code.<br>\n\n### Application to deep learning\nThe Gumbel-max trick finds significant application in deep learning, particularly in sampling from categorical distributions during optimization. Below are two common issues in deep learning that the trick helps address.\n\n+ #### Issue-1\nOptimizing categorical probabilities $p_k$ often requires solving a constrained problem where $\\sum_k p_k = 1$ which is hard to optimize using gradient descent. To overcome, one simple is to reparameterize $p_k$ using another variable $\\theta_k \\in \\mathbb{R}$ through softmax. \n\n\\begin{align}\np_k = \\frac{\\exp(\\theta_k)}{\\sum_j \\exp(\\theta_j)}~\\mathrm{where}~\\theta_k \\in \\mathbb{R}.\n\\end{align}\n\n$\\theta_k$ could be the logits from a neural network. This formulation allows for unconstrained optimization since the logits can take any real value.\nHowever, in some scenarios, normalization can be either computationally expensive or may not be possible. You can think of a streaming automatic speech recognition system, where not all the logits are available. Gumbel-Max trick can overcome this. Following the proof along the similar lines as above, it can be shown that $\\underset{k}{\\text{argmax}}\\{\\theta_k + G_k\\} \\sim \\text{Cat}(p_k)$ (You can do it as an exercise). This result remains valid for each $\\theta_k$'s. \n\n+ #### Issue-2\nNeural networks typically rely on gradient-based optimization, which requires all operations to be differentiable. Both the inverse CDF method and the Gumbel-max trick involve non-differentiable operations.<br>\n  + The inverse CDF method introduces discontinuities when selecting categories based on cumulative probabilities.<br>\n  + The Gumbel-max trick involves the $\\text{argmax}$ operation, which is non-differentiable.\n\nWhile many researchers have proposed relaxation for the differentiability of both methods, the Softmax variant of the Gumbel-max trick \"Gumbel-softmax trick\" is often used. It replaces the $\\text{argmax}$ with a differentiable approximation (softmax) that allows gradients to flow through the sampling process, making it suitable for backpropagation during neural network training. We leave this for future discussion (reference:\n<a href=\"https://arxiv.org/pdf/1611.01144\" style=\"text-decoration: none; color: blue;\" target=\"_blank\">Gumbel-Softmax</a>).\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n\n> Pre-requisite: Probability theory\n\nThe Gumbel-max trick is useful technique for sampling from a discrete distribution, especially when built-in statistical functions are unavailable. Although many statistical software packages can handle a wide range of distributions, there are situations where you need to sample from distributions that may not be explicitly supported.\n\nIn this blog, we will discuss the Gumbel-max trick, it's mathematical proof, and a provide simulation example. Before diving into the trick, let's do something easy first. We will briefly revist the inverse cumulative distribution function (CDF) method for smampling form a distribution, which applies to both continuous and discrete distributions. In contrast, the Gumbel-max trick is specifically applicable to the discrete distributions.\n\n### Inverse CDF method\n**Definition**: The CDF of a random variable $Z$ is given by $F_Z(z) = P(Z \\leq z)~\\forall~z$ where $P(\\cdot)$ denotes the probability. The CDF is a non-decreasing function defined on the real line $\\mathbb{R}$.<br>\n**Definition**: For a non-decreasing function $F$ on $\\mathbb{R}$, the generalized inverse is defined as $F^{-1}(u) = min\\{z: F(z) \\geq u\\}, u \\in \\mathbb{R}$. For a continuous random variable the inequality \"$\\geq$\" become equality \"$=$\".<br>\nSuppose that we want to sample $Z \\sim P(z)$, and we have access to inverse CDF of $Z$, then<br>\n**Lemma**:<br>\nIf $U \\sim \\text{Uniform}(0, 1)$, then $Z = F_Z^{-1}(U)$ is a simulation from $P(z)$.<br>\n***Proof***: Consider $P(Z \\leq z) = P(F_Z^{-1}(U) \\leq z) = P(U \\leq F_Z(z)) = \\int_{0}^{F_Z(z)} 1~\\text{d}u =  F_Z(z)$. Hence, $Z$ follows the desired distribution $P(z)$.\nWe can observe that the inverse CDF method can be used to generate samples of $Z$ through the transformation of a uniform random variable where the transformation is the inverse CDF of $Z$ itself. For most of the statistical softwares it is easier to generate samples from a uniform distribution.\n\n#### A Coding example\nSuppose $Z$ is a categorical random variable that is $Z$ can take values from $\\{1, 2, \\dots, K\\}$ with probabilities $P(Z=k)=p_k$ where $K$ denotes the categories. We would like to generate various realizations (samples) of $Z$ (e.g. $\\{Z_1, Z_2, \\cdots, Z_n\\}$) from this distribution. \nWe can employ the inverse CDF method. Please refer to the Python code example for more clarity, come back, and read again.<br>\n \n### Gumbel-max trick\n**Definition**: The Gumbel distribution of a random variable $X \\in \\mathbb{R}$ is given by $f_X(x) = \\frac{1}{\\beta}\\exp(-(x - \\mu) - \\exp(-(x - \\mu)/\\beta))$, the CDF is $F_X(x) = P(X \\leq x) = \\exp(-\\exp(-(x - \\mu)/\\beta))$, and the inverse CDF is $-\\log(\\log(x - \\mu)/\\beta)$. To the scope of this discussion, it is enough to consider $\\mu=0, \\beta=1$, that is $X \\sim \\text{Gumbel}(0, 1)$.<br>\nThe Gumbel-max trick converts the sampling problem into an optimization problem and the key result is as follows.<br>\n\\begin{align}\n\\tag{1}\nI = \\underset{k}{\\text{argmax}}~(\\log(p_k) + G_k) \\sim \\text{Categorical}(p_k), 1 \\leq k \\leq K\n\\end{align}\nwhere $G_k \\sim \\text{Gumbel}(0, 1)$ are Gumbel-distributed i.i.d random variables, and $I\\in \\mathbb{Z}$ is a random variable because of $G_k$. Eq. (1) states that the index $I$ follows the desired categorical distribution (see proof), this index is obtained by adding samples from the Gumbel distribution (the Gumbel noise) to the log probabilities and choosing the index for which the quantity is maximum. <br>\n***Proof***: Let $X_k = \\log(p_k) + G_k \\in \\mathbb{R}$. If the solution of Eq. (1) is the index $k$, then the event $\\{I = k\\}$ is equivalent to the event $\\{X_k > X_j~\\forall j \\neq k\\}$ in the probability space. We can further translate it to another equivalent event \"$\\{\\text{sum over all}~x~(X_k = x, X_j < x~\\forall j \\neq k)\\}$\". Please note that $G_k$'s are i.i.d and consequently $X_k$'s are also i.i.d.\nHence in probability space, we have<br>\n\\begin{align}\nP(I = k) &= P(X_k > X_j~\\forall k \\neq j) = \\int P(X_k = x, X_j < x~\\forall j \\neq k)~\\mathrm{d}x \\\\ \\nonumber\n&=\\int P(X_k = x) P(X_j < x~\\forall~j \\neq k|X_k = x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(X_k = x) P(X_j < x~\\forall~j \\neq k)~\\mathrm{d}x =  \\int P(X_k = x) \\prod_{j \\neq k} P(X_j < x)~\\mathrm{d}x \\\\ \\nonumber\n&= \\int P(G_k = x - log(p_k)) \\prod_{j \\neq k} P(G_j < x - \\log(p_j))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k) - x - \\exp(\\log(p_k) - x)) \\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int \\exp(\\log(p_k))\\exp(-x)\\exp(-\\exp(\\log(p_k) - x))\\prod_{j \\neq k} \\exp(-\\exp(\\log(p_j) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x)\\prod_{k} \\exp(-\\exp(\\log(p_k) - x))~\\mathrm{d}x \\\\ \\nonumber\n&= \\int p_k \\exp(-x) \\prod_{k} \\exp(-p_k\\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int \\exp(-x) \\exp(-\\sum_{k} p_k \\exp(-x)))~\\mathrm{d}x \\\\ \\nonumber\n&= p_k \\int_{-\\infty}^{+\\infty} \\exp(-x) \\exp(-1\\exp(-x)))~\\mathrm{d}x = p_k\n\\end{align}\nThis completes the proof. Hence the random variable $I \\sim \\text{Cat}(p_k)$. To generate samples using this method, you only need to sample Gumbel-distributed variables $G_k$ and find the index that maximizes $\\log(p_k) + G_k$. In contrast to inverse CDF method, this avoids computing the CDF or performing a binary search.\nTo sample Gumbel-distributed variables, you can start with a uniform random variable and transform it using the inverse Gumbel CDF  $(-\\log(-\\log(-x)))$ which is a closed-form expression, the resulting random variable will follow the Gumbel distribution.\nWe can now generate various realizations of $I$, i.e. $\\{I_1, I_2, \\dots, I_n\\}$ by adding each time a different realization of Gumbel noise to the log-probabilities and picking up on the maximum index. Please refer to the Python code.<br>\n\n### Application to deep learning\nThe Gumbel-max trick finds significant application in deep learning, particularly in sampling from categorical distributions during optimization. Below are two common issues in deep learning that the trick helps address.\n\n+ #### Issue-1\nOptimizing categorical probabilities $p_k$ often requires solving a constrained problem where $\\sum_k p_k = 1$ which is hard to optimize using gradient descent. To overcome, one simple is to reparameterize $p_k$ using another variable $\\theta_k \\in \\mathbb{R}$ through softmax. \n\n\\begin{align}\np_k = \\frac{\\exp(\\theta_k)}{\\sum_j \\exp(\\theta_j)}~\\mathrm{where}~\\theta_k \\in \\mathbb{R}.\n\\end{align}\n\n$\\theta_k$ could be the logits from a neural network. This formulation allows for unconstrained optimization since the logits can take any real value.\nHowever, in some scenarios, normalization can be either computationally expensive or may not be possible. You can think of a streaming automatic speech recognition system, where not all the logits are available. Gumbel-Max trick can overcome this. Following the proof along the similar lines as above, it can be shown that $\\underset{k}{\\text{argmax}}\\{\\theta_k + G_k\\} \\sim \\text{Cat}(p_k)$ (You can do it as an exercise). This result remains valid for each $\\theta_k$'s. \n\n+ #### Issue-2\nNeural networks typically rely on gradient-based optimization, which requires all operations to be differentiable. Both the inverse CDF method and the Gumbel-max trick involve non-differentiable operations.<br>\n  + The inverse CDF method introduces discontinuities when selecting categories based on cumulative probabilities.<br>\n  + The Gumbel-max trick involves the $\\text{argmax}$ operation, which is non-differentiable.\n\nWhile many researchers have proposed relaxation for the differentiability of both methods, the Softmax variant of the Gumbel-max trick \"Gumbel-softmax trick\" is often used. It replaces the $\\text{argmax}$ with a differentiable approximation (softmax) that allows gradients to flow through the sampling process, making it suitable for backpropagation during neural network training. We leave this for future discussion (reference:\n<a href=\"https://arxiv.org/pdf/1611.01144\" style=\"text-decoration: none; color: blue;\" target=\"_blank\">Gumbel-Softmax</a>).\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"output-file":"gumbel_max.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","comments":{"utterances":{"repo":"Jitendradhiman/jkdblog","issue-term":"title","label":"💬","theme":"github-light"}},"theme":{"light":["cosmo","../../theme.scss"],"dark":["darkly","../../theme.scss"]},"toc-location":"left","title-block-banner":false,"date":"Sep 25, 2024","page-layout":"article","author":"Jitendra K. Dhiman","title":"Gumbel-Max Trick"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}